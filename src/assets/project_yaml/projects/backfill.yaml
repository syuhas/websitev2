- id: backfill
  title: Optimizing Database Backfill with Python Threads in AWS Lambda
  subtitle: Event-driven architecture in AWS with EventBridge, Lambda, SQS, and S3.
  github: https://github.com/syuhas/awsdash-lambda
  description: |
      Developing an event-driven architecture in AWS with serverless microservices
  listIcon: /assets/project-images/backfill-images/backfill_icon1.png
  titleIcons:
      - /assets/project-images/backfill-images/backfill_icon1.png
      - /assets/project-images/backfill-images/backfill_icon2.png
      - /assets/project-images/backfill-images/backfill_icon3.png
  sections:
      - title: Overview
        tabTitle: Overview
        subsections:
            - content: ""
              imgs:
                  - /assets/project-images/backfill-images/backfill_diagram1.png

            - content: |
                  Recently I've been exploring the different aspects of creating backend automation for dashboards and reporting for resources like AWS services and Jenkins build data. For my dataset, I chose to gather metadata for all buckets and objects in my accounts to create a utilization and cost dashboard for my S3 resources.<br><br>

                  In my <a href="/projects/event">previous project</a>, I described the event-driven architecture I built to track future events for gathering my S3 data. This is great for new data but does not address the thousands of historical objects and buckets I already have in my accounts that have not yet been processed and stored in my <strong style="color: blue">PostgreSQL</strong> database.<br><br>

                  This is where <strong>backfilling</strong> as a concept comes into play. Since I already have data in my S3 buckets, I need a way to add potentially thousands of objects into the database to fill in the missing historical data. Given that these can be large datasets and take a long time to process, I wanted to optimize my script as much as possible to handle the load efficiently.<br><br>

              imgs:
                  - /assets/project-images/backfill-images/backfill_diagram2.png

            - content: |
                  It just so happens I had been researching and learning about the nuances of the <strong style="color: blue">Global Interpreter Lock (GIL)</strong> and how this works with threading and multiprocessing in Python. The bottleneck here seemed to be the database operations, and threading is a good choice for this use case because it allows me to handle multiple I/O-bound tasks concurrently by overlapping the write and delete operations. While the nature of Python's GIL means that threading is not truly concurrent, Python's ability to release the GIL during I/O waits makes it still very useful in optimizing my backfill.<br><br>

                  To further enhance efficiency, I combined <strong>batch processing</strong> with Python's <strong style="color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;">ThreadPoolExecutor</strong> to optimize database write and delete operations. By processing data in manageable chunks, the script avoids overwhelming the database and reduces resource contention(when multiple threads are trying to access the resource simultaneously). Each batch is processed independently, ensuring stability and scalability, and errors are handled gracefully to maintain data integrity. This approach balances performance and reliability, making it well-suited for processing large datasets efficiently. For existing object comparisons, I used objects hashes to determine if an object needs to be updated in the database which is much faster that comparing object metadata indivuidually.<br><br>

                  Given the size of the dataset and the optimizations I made, I was able to use <strong style="color: blue">AWS Lambda</strong> as a serverless solution to run my script. For larger datasets, a service like <strong style="color: blue">AWS Batch</strong> is much more suitable for processes that could run for 24 hours or more. For my purposes, however, Lambda works just fine as the average runtime is well under the 15 minute limit. In the following sections, I will walk through the code and discuss the key design decisions.

              imgs:

      - title: Code Introduction
        tabTitle: Code
        subsections:
            - content: |
                  Below is the script in its entirety. The following optimizations have been added to significantly improve the speed and efficiency of the code:<br>

              listItems:
                  - "<span style='color: blue'>Threading with ThreadPoolExecutor</span> for concurrent database operations"
                  - "<span style='color: blue'>Batch processing</span> for database write and delete operations"
                  - "<span style='color: blue'>Hash comparison</span> for efficient object updates"

              imgs:

            - content: |
                  (<a href="https://github.com/syuhas/awsdash-lambda/blob/main/backfill/lambda_function.py" target="_blank">Link to the Lambda code in GitHub</a>)<br>

              code: |
                  <span style="color: rgb(0, 255, 0);">
                  from typing import List, Dict, Union
                  import boto3
                  import json
                  import boto3.session
                  from loguru import logger
                  from sqlalchemy import create_engine, select
                  from sqlalchemy.orm import sessionmaker, declarative_base, relationship, scoped_session
                  from sqlalchemy import Column, Integer, String, ForeignKey, DECIMAL, BigInteger
                  from botocore.exceptions import ClientError
                  from concurrent.futures import ThreadPoolExecutor
                  import time
                  from itertools import islice
                  </span>

                  <span style="color: rgb(0, 255, 0);">##################################### Account Lookup Dictionary #################################################</span>

                  account_lookup = [
                      {
                          'sdlc': 'prod',
                          'account_id': '551796573889',
                          'region': 'us-east-1',
                          'role_arn': 'arn:aws:iam::551796573889:role/jenkinsAdminXacnt'
                      },
                      {
                          'sdlc': 'dev',
                          'account_id': '061039789243',
                          'region': 'us-east-1',
                          'role_arn': 'arn:aws:iam::061039789243:role/jenkinsAdminXacnt'
                      }
                  ]

                  <span style="color: rgb(0, 255, 0);">##################################### Database Class Definitions ################################################</span>

                  Base = declarative_base()
                  class S3BUCKETS(Base):
                      __tablename__ = 's3'
                      id = Column(Integer, primary_key=True)
                      account_id = Column(String)
                      bucket = Column(String)
                      totalSizeBytes = Column(BigInteger)
                      totalSizeKb = Column(DECIMAL)
                      totalSizeMb = Column(DECIMAL)
                      totalSizeGb = Column(DECIMAL)
                      costPerMonth = Column(DECIMAL)
                      objects = relationship('S3BUCKETOBJECTS', backref='s3objects')
                      def __repr__(self):
                          return f'Bucket {self.bucket}'

                  class S3BUCKETOBJECTS(Base):
                      __tablename__ = 's3objects'
                      id = Column(Integer, primary_key=True)
                      bucket_id = Column(Integer, ForeignKey('s3.id'))
                      bucket = Column(String)
                      key = Column(String)
                      sizeBytes = Column(BigInteger)
                      sizeKb = Column(DECIMAL)
                      sizeMb = Column(DECIMAL)
                      sizeGb = Column(DECIMAL)
                      costPerMonth = Column(DECIMAL)

                  <span style="color: rgb(0, 255, 0);">##################################### Lambda Handler ############################################################</span>

                  def lambda_handler(event, context):
                      logger.info("Starting backfill process...")
                      start_time = time.time()

                      for account in account_lookup:


                          logger.info("Retrieving existing buckets for {}...", account['account_id'])

                          session = getAccountSession(account)

                          buckets = getBucketsData(session)

                          logger.info("Backfilling database for {}...", account['account_id'])

                          backfillDatabase(buckets, account['account_id'])
                      
                      end_time = time.time()

                      logger.info(f"Execution time: {end_time - start_time} seconds")

                  <span style="color: rgb(0, 255, 0);">##################################### Main Backfill Function ####################################################</span>

                  def backfillDatabase(buckets: List[dict], account_id: str) -> dict:
                      try:
                          current_keys = set()
                          objects_to_add = []
                          session = getDatabaseSession()
                          existing_db_buckets = {b.bucket: b.id for b in session.query(S3BUCKETS).filter(S3BUCKETS.account_id == account_id).all()}
                          existing_db_objects = {(obj.bucket_id, obj.key): {
                                                      "id": obj.id,
                                                      "hash": hash((obj.sizeBytes, obj.costPerMonth))
                          } for obj in session.query(S3BUCKETOBJECTS).join(S3BUCKETS, S3BUCKETS.id == S3BUCKETOBJECTS.bucket_id).filter(S3BUCKETS.account_id == account_id).all()}

                          try:
                              logger.info('Processing any new buckets and objects...')
                              with ThreadPoolExecutor() as executor:
                                  futures = [
                                      executor.submit(processBucket, bucket, existing_db_buckets, existing_db_objects, account_id, current_keys, objects_to_add) for bucket in buckets
                                  ]
                                  for future in futures:
                                      future.result()

                              current_buckets = {bucket['bucket'] for bucket in buckets}

                          except Exception as e:
                              logger.exception(e)
                              session.rollback()
                              return {
                                  'statusCode': 500,
                                  'body': json.dumps(f'An error occurred: {e}')
                              } 
                          logger.info('Buckets processed.')


                          logger.info('Checking for objects to add...')
                          try:
                              if objects_to_add:
                                  logger.info('Adding new objects')
                                  logger.info(f'Batching {len(objects_to_add)} objects to add to the database...')
                                  logger.info(f'Batch size: 100')
                                  batch_size = 100
                                  objects_to_add_batches = chunked_iterable([obj for obj in objects_to_add], batch_size)
                                  with ThreadPoolExecutor() as executor:
                                      futures = [
                                          executor.submit(addObjects, batch, count)
                                          for count, batch in enumerate(objects_to_add_batches)
                                      ]
                                      
                                      for future in futures:
                                          future.result()

                          except Exception as e:
                              logger.exception(e)
                              session.rollback()
                              return {
                                  'statusCode': 500,
                                  'body': json.dumps(f'An error occurred: {e}')
                              }


                          logger.info('Checking for objects to delete...')
                          objects_to_delete = set(existing_db_objects.keys()) - current_keys

                          try:
                              if objects_to_delete:
                                  logger.info(f'Batching {len(objects_to_delete)} objects for deletion')
                                  logger.info(f'Batch size: 100')
                                  batch_size = 100
                                  logger.info(f'Number of batches: {len(objects_to_delete) // batch_size}')
                                  objects_to_delete_batches = chunked_iterable([existing_db_objects[obj]['id'] for obj in objects_to_delete], batch_size)
                                  with ThreadPoolExecutor() as executor:
                                      futures = [
                                          executor.submit(deleteObjects, batch, count)
                                          for count, batch in enumerate(objects_to_delete_batches)
                                      ]
                                      
                                      for future in futures:
                                          future.result()

                          except Exception as e:
                              logger.exception(e)
                              session.rollback()
                              return {
                                  'statusCode': 500,
                                  'body': json.dumps(f'An error occurred: {e}')
                              }
                          
                          logger.info('Checking for buckets to delete...')
                          buckets_to_delete = set(existing_db_buckets.keys()) - current_buckets
                          if buckets_to_delete:
                              for bucket in buckets_to_delete:
                                  logger.info(f'Deleting bucket from database {bucket}')
                                  deleteBucket(session, existing_db_buckets[bucket])

                      except Exception as e:
                          session.rollback()
                          return {
                              'statusCode': 500,
                              'body': json.dumps(f'An error occurred: {e}')
                          }
                      finally:
                          session.commit()
                          session.close()


                  <span style="color: rgb(0, 255, 0);">##################################### Bucket Processing Function ################################################</span>

                  def processBucket(bucket: Dict, existing_db_buckets: Dict, existing_db_objects: Dict, account_id: str, current_keys: set, objects_to_add: list[S3BUCKETOBJECTS]) -> None:
                      bucket_id = existing_db_buckets.get(bucket['bucket'])
                      try:
                          thread_session = getThreadsafeDatabaseSession()
                          if bucket_id:
                              if bucketNeedsUpdate(thread_session, bucket_id, bucket, account_id):
                                  modifyBucket(thread_session, bucket_id, bucket, account_id)
                                  logger.info(f'Bucket {bucket["bucket"]} updated.')
                          else:
                              logger.info(f'Adding bucket {bucket["bucket"]} to the database.')
                              bucket_id = addBucket(thread_session, bucket, account_id)

                          for obj in bucket['objects']:
                              object_key = (bucket_id, obj['key'])
                              if object_key in existing_db_objects:
                                  existing_object = existing_db_objects[object_key]
                                  if objectNeedsUpdate(existing_object, obj):
                                      modifyObject(thread_session, existing_object['id'], obj)
                              else:
                                  objects_to_add.append({
                                      'bucket_id': bucket_id,
                                      'obj': obj
                                  })

                              current_keys |= ({(bucket_id, obj['key'])})

                          logger.info(f'Existing object hashes in {bucket["bucket"]} checked.')

                          thread_session.commit()
                          thread_session.remove()

                      except Exception as e:
                          logger.exception(e)




                  <span style="color: rgb(0, 255, 0);">#################################### Helper Functions for Buckets ##################################################</span>

                  def bucketNeedsUpdate(session, bucket_id: int, bucket: Dict, account_id: str) -> bool:
                      existing_bucket = session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).one()
                      return (
                          existing_bucket.totalSizeBytes != bucket['totalSizeBytes']
                      )

                  def modifyBucket(session, bucket_id: int, bucket: Dict, account_id: str) -> None:
                      session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).update({
                          "account_id": account_id,
                          "totalSizeBytes": bucket["totalSizeBytes"],
                          "totalSizeKb": bucket["totalSizeKb"],
                          "totalSizeMb": bucket["totalSizeMb"],
                          "totalSizeGb": bucket["totalSizeGb"],
                          "costPerMonth": bucket["costPerMonth"],
                      })
                      session.commit()


                  def addBucket(session: sessionmaker, bucket: Dict, account_id: str) -> str:
                      new_bucket = S3BUCKETS(
                          account_id=account_id,
                          bucket=bucket['bucket'],
                          totalSizeBytes=bucket['totalSizeBytes'],
                          totalSizeKb=bucket['totalSizeKb'],
                          totalSizeMb=bucket['totalSizeMb'],
                          totalSizeGb=bucket['totalSizeGb'],
                          costPerMonth=bucket['costPerMonth']
                      )
                      session.add(new_bucket)
                      session.commit()
                      return new_bucket.id

                  def deleteBucket(session: sessionmaker, bucket_id: int) -> None:
                      session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.bucket_id == bucket_id).delete()
                      session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).delete()
                      session.commit()

                  <span style="color: rgb(0, 255, 0);">##################################### Helper Functions for Objects #################################################</span>

                  def objectNeedsUpdate(existing_object, object: Dict) -> bool:
                      return (
                          existing_object['hash'] != object['hash']
                      )

                  def modifyObject(session: sessionmaker, object_id: int, object: Dict) -> None:
                      session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.id == object_id).update({
                          "sizeBytes": object["sizeBytes"],
                          "sizeKb": object["sizeKb"],
                          "sizeMb": object["sizeMb"],
                          "sizeGb": object["sizeGb"],
                          "costPerMonth": object["costPerMonth"]
                      })
                      session.commit()

                  def addObjects(batch: list[S3BUCKETOBJECTS], count: int) -> None:
                      thread_session = getThreadsafeDatabaseSession()
                      logger.info(f'Adding batch #{count} to the database...')
                      for obj in batch:
                          # logger.info(f'Adding object {obj["obj"]["key"]} to the database.')
                          new_object = S3BUCKETOBJECTS(
                              bucket_id=obj['bucket_id'],
                              bucket=obj['obj']['bucket'],
                              key=obj['obj']['key'],
                              sizeBytes=obj['obj']['sizeBytes'],
                              sizeKb=obj['obj']['sizeKb'],
                              sizeMb=obj['obj']['sizeMb'],
                              sizeGb=obj['obj']['sizeGb'],
                              costPerMonth=obj['obj']['costPerMonth']
                          )
                          thread_session.add(new_object)
                      thread_session.commit()
                      thread_session.remove()
                      logger.info(f'Batch #{count} added to the database.')

                  def deleteObjects(object_ids: list[int], count: int) -> None:
                      thread_session = getThreadsafeDatabaseSession()
                      logger.info(f'Deleting batch #{count}')
                      thread_session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.id.in_(object_ids)).delete(synchronize_session='fetch')
                      logger.info(f'Batch #{count} deleted.')
                      thread_session.commit()
                      thread_session.remove()

                  def chunked_iterable(iterable, size):
                      iterator = iter(iterable)
                      for first in iterator:
                          yield [first] + list(islice(iterator, size - 1))


                  <span style="color: rgb(0, 255, 0);">##################################### AWS Helper Functions #################################################</span>

                  def getAccountSession(account: dict) -> boto3.session.Session:
                      session = boto3.Session()
                      sts = session.client('sts')
                      response = sts.assume_role(
                          RoleArn=account['role_arn'],
                          RoleSessionName='s3-backfill',
                          DurationSeconds=900
                      )
                      credentials = response['Credentials']
                      account_session = boto3.Session(
                          aws_access_key_id=credentials['AccessKeyId'],
                          aws_secret_access_key=credentials['SecretAccessKey'],
                          aws_session_token=credentials['SessionToken'],
                          region_name=account['region']
                      )
                      return account_session

                  def getBucketsData(session: boto3.session.Session) -> list:
                      logger.info('Retrieving S3 bucket data from AWS...')
                      bucket_list = []

                      s3 = session.client('s3')
                      buckets = s3.list_buckets()

                      for bucket in buckets['Buckets']:
                          # if bucket['Name'].startswith('aws-cloudtrail'):
                          bucket_dict = {
                              'bucket': bucket['Name'],
                              'totalSizeBytes': 0,
                              'totalSizeKb': 0,
                              'totalSizeMb': 0,
                              'totalSizeGb': 0,
                              'costPerMonth': 0,
                              'objects': []
                          }
                          
                          paginator = s3.get_paginator('list_objects_v2')
                      
                          object_list = []
                      
                          total_bucket_cost = 0
                      
                          object_iterator = paginator.paginate(Bucket=bucket['Name'])
                          for page in object_iterator:
                              if 'Contents' in page:
                                  for obj in page['Contents']:
                                      object_dict = {
                                          'key': obj['Key'],
                                          'bucket': bucket['Name'],
                                          'sizeBytes': obj['Size'],
                                          'sizeKb': round(obj['Size'] / 1024, 2),
                                          'sizeMb': round(obj['Size'] / (1024 * 1024), 2),
                                          'sizeGb': round(obj['Size'] / (1024 * 1024 * 1024), 4),
                                      }
                                      object_dict['costPerMonth'] = object_dict['sizeGb'] * 0.023
                                      object_dict['hash'] = hash((object_dict['sizeBytes'], object_dict['costPerMonth']))
                                      total_bucket_cost = total_bucket_cost + object_dict['costPerMonth']
                                      object_list.append(object_dict)

                          bucket_dict['totalSizeBytes'] = sum([obj['sizeBytes'] for obj in object_list])
                          bucket_dict['totalSizeKb'] = round(sum([obj['sizeKb'] for obj in object_list]), 4)
                          bucket_dict['totalSizeMb'] = round(sum([obj['sizeMb'] for obj in object_list]), 4)
                          bucket_dict['totalSizeGb'] = round(sum([obj['sizeGb'] for obj in object_list]), 4)
                          bucket_dict['costPerMonth'] = round(total_bucket_cost, 6)
                          bucket_dict['objects'] = object_list
                          bucket_list.append(bucket_dict)

                      logger.info('S3 bucket data retrieved.')
                      return bucket_list

                  def getDatabaseCredentials() -> dict:
                      secret_id = "arn:aws:secretsmanager:us-east-1:061039789243:secret:rds!db-555390f8-60f2-4d37-ad75-e63d8f0cbfa9-0s9oyX"
                      region = "us-east-1"
                      session = boto3.session.Session()
                      client = session.client('secretsmanager', region_name=region)

                      try:
                          secret_response = client.get_secret_value(SecretId=secret_id)
                          secret = secret_response['SecretString']
                          json_secret = json.loads(secret)
                          credentials = {
                              'username': json_secret['username'],
                              'password': json_secret['password']
                          }
                          return credentials
                      except ClientError as e:
                          raise e

                  <span style="color: rgb(0, 255, 0);">##################################### Database Helper Functions #################################################</span>

                  def getEngine() -> create_engine:
                      credentials = getDatabaseCredentials()
                      engine = create_engine(
                          f'postgresql://{credentials["username"]}:{credentials["password"]}@resources.czmo2wqo0w7e.us-east-1.rds.amazonaws.com:5432'
                      )
                      return engine

                  def getDatabaseSession() -> sessionmaker:
                      engine = getEngine()
                      Session = sessionmaker(bind=engine)
                      session = Session()
                      return session

                  def getThreadsafeDatabaseSession() -> scoped_session:
                      engine = getEngine()
                      session_factory = sessionmaker(bind=engine)
                      session = scoped_session(session_factory)
                      return session

                  def getBuckets(account_id: str) -> list:
                      session = getDatabaseSession()
                      result = session.query(S3BUCKETS).filter(S3BUCKETS.account_id == account_id).all()
                      session.close()
                      return result

                  def getObjectsForBucket(bucket_id: int) -> list:
                      session = getDatabaseSession()
                      result = session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.bucket_id == bucket_id).all()
                      session.close()
                      return result
              imgs:

            - content: |
                  I am using a combination of boto3 and SQLAlchemy ORM (Object Relational Mapping) for the bulk of the backfill operations. The threading libraries are built in to Python as well as the itertools used to manage my batch sizes when processing object data chunks. To run in Lambda, AWS-specific psychopg libraries are available to support PostgreSQL dialect in SQLAlchemy. Additionally, I prefer Loguru for logging so I need to package the following imports for my function. <br><br>

                  <span style="opacity: 50%">(boto3 is optional here as Lambda functions have this library pre-installed. But to be safe and ensure fine-grained control over my dependencies, I include it for the sake of completeness.)</span>
                  <br><br>

              imgs:
                  - /assets/project-images/backfill-images/backfill1_1.png

      - title: Structure of the Code
        tabTitle: Structure
        subsections:
            - content: |
                  Setting up a CloudTrail trail is a crucial first step in this event-driven architecture. The primary purpose of this trail is to capture and log management events for S3 buckets, enabling integration with EventBridge. This integration allows EventBridge to read these events and trigger workflows based on bucket-level activities such as creating or deleting a bucket.<br><br>
                  In this setup, the CloudTrail trail is configured to log management events, with particular emphasis on Write API operations. These logs are delivered to a designated S3 bucket, where they can be accessed and processed. By enabling this trail, I ensure that EventBridge has the necessary visibility into bucket-level actions, which forms the foundation of the bucket pipeline.<br><br>
              imgs:
                  - /assets/project-images/event-images/event1_1.png
                  - /assets/project-images/event-images/event1_2.png
            - content: |
                  Below is the bucket that the CloudTrail logs are delivered to. This trail is enabled organization-wide, so all S3 management events from all accounts in the org are captured. These logs will be used by EventBridge and filtered by CreateBucket and DeleteBucket events.
              imgs:
                  - /assets/project-images/event-images/event1_3.png

      - title: Summary
        tabTitle: Summary
        subsections:
            - content: |
                  This project was a valuable learning experience that brought together multiple AWS services into a cohesive event-driven architecture. It began with the challenge of automating the enrollment of S3 buckets and evolved into a full-scale pipeline capable of processing both bucket-level and object-level events efficiently. From setting up CloudTrail and EventBridge to leveraging SQS and Lambda functions, every step of the pipeline presented opportunities to deepen my understanding of event-driven design and AWS integration.

                  <br><br>

                  One of the most rewarding aspects of the project was working with Lambda functions to process and enrich event data. This included calculating storage metrics and costs for S3 objects and enrolling buckets for additional event notifications. Integrating PostgreSQL to store the processed data added a layer of persistence and visibility to the workflow, allowing for more meaningful insights into bucket usage and object-level changes.

                  <br><br>

                  Deploying the entire architecture with Jenkins and Terraform was another critical milestone. Building a CI/CD pipeline not only streamlined the deployment process but also ensured infrastructure consistency and reliability. Incorporating a bash script to package Lambda functions and leveraging Terraformâ€™s state management and hashing features to track changes further emphasized the importance of automation and reproducibility.

                  <br><br>

                  Reflecting on the project, I gained significant experience in designing and deploying event-driven architectures. I also learned the importance of careful planning when integrating multiple AWS services to ensure seamless communication and error handling. This project demonstrated the potential for scalability and highlighted areas for future exploration, such as optimizing notification workflows or adding advanced monitoring capabilities.

                  <br><br>

                  Overall, this project was not only a technical accomplishment but also a major step forward in understanding how to build efficient, scalable, and cost-effective serverless solutions. It has laid the groundwork for future enhancements and provided a solid foundation for tackling more complex event-driven systems.
              imgs:
