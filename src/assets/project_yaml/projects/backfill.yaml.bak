- id: backfill
  title: Optimizing Database Backfill with Python Threads in AWS Lambda
  subtitle: Event-driven architecture in AWS with EventBridge, Lambda, SQS, and S3.
  github: https://github.com/syuhas/awsdash-lambda
  description: |
      Developing an event-driven architecture in AWS with serverless microservices
  listIcon: /assets/project-images/backfill-images/backfill_icon1.png
  titleIcons:
    - /assets/project-images/backfill-images/backfill_icon1.png
    - /assets/project-images/backfill-images/backfill_icon2.png
    - /assets/project-images/backfill-images/backfill_icon3.png
  sections:
    - title: Overview
      tabTitle: Overview
      subsections:
        - content: ""
          imgs:
            - /assets/project-images/backfill-images/backfill_diagram1.png

        - content: |
              Recently I've been exploring the different aspects of creating backend automation for dashboards and reporting for resources like AWS services and Jenkins build data. For my dataset, I chose to gather metadata for all buckets and objects in my accounts to create a utilization and cost dashboard for my S3 resources.<br><br>

              In my <a href="/projects/event">previous project</a>, I described the event-driven architecture I built to track future events for gathering my S3 data. This is great for new data but does not address the thousands of historical objects and buckets I already have in my accounts that have not yet been processed and stored in my <strong style="color: blue">PostgreSQL</strong> database.<br><br>

              This is where <strong>backfilling</strong> as a concept comes into play. Since I already have data in my S3 buckets, I need a way to add potentially thousands of objects into the database to fill in the missing historical data. Given that these can be large datasets and take a long time to process, I wanted to optimize my script as much as possible to handle the load efficiently.<br><br>

          imgs:
            - /assets/project-images/backfill-images/backfill_diagram2.png

        - content: |
              It just so happens I had been researching and learning about the nuances of the <strong style="color: blue">Global Interpreter Lock (GIL)</strong> and how this works with threading and multiprocessing in Python. The bottleneck here seemed to be the database operations, and threading is a good choice for this use case because it allows me to handle multiple I/O-bound tasks concurrently by overlapping the write and delete operations. While the nature of Python's GIL means that threading is not truly concurrent, Python's ability to release the GIL during I/O waits makes it still very useful in optimizing my backfill.<br><br>

              To further enhance efficiency, I combined <strong>batch processing</strong> with Python's <strong style="color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;">ThreadPoolExecutor</strong> to optimize database write and delete operations. By processing data in manageable chunks, the script avoids overwhelming the database and reduces resource contention(when multiple threads are trying to access the resource simultaneously). Each batch is processed independently, ensuring stability and scalability, and errors are handled gracefully to maintain data integrity. This approach balances performance and reliability, making it well-suited for processing large datasets efficiently. For existing object comparisons, I used objects hashes to determine if an object needs to be updated in the database which is much faster that comparing object metadata indivuidually.<br><br>

              Given the size of the dataset and the optimizations I made, I was able to use <strong style="color: blue">AWS Lambda</strong> as a serverless solution to run my script. For larger datasets, a service like <strong style="color: blue">AWS Batch</strong> is much more suitable for processes that could run for 24 hours or more. For my purposes, however, Lambda works just fine as the average runtime is well under the 15 minute limit. In the following sections, I will walk through the code and discuss the key design decisions.

          imgs:


    - title: Code Introduction
      tabTitle: Code
      subsections:
        - content: |
              Below is the script in its entirety. The following optimizations have been added to significantly improve the speed and efficiency of the code:<br>

          listItems:
            - text: "<span style='color: blue'>Threading with ThreadPoolExecutor</span> for concurrent database operations"
            - text: "<span style='color: blue'>Batch processing</span> for database write and delete operations"
            - text: "<span style='color: blue'>Hash comparison</span> for efficient object updates"

          imgs:


        - content: |
              (<a href="https://github.com/syuhas/awsdash-lambda/blob/main/backfill/lambda_function.py" target="_blank">Link to the Lambda code in GitHub</a>)<br>

          code: |
              <span style="color: rgb(0, 255, 0);">
              from typing import List, Dict, Union
              import boto3
              import json
              import boto3.session
              from loguru import logger
              from sqlalchemy import create_engine, select
              from sqlalchemy.orm import sessionmaker, declarative_base, relationship, scoped_session
              from sqlalchemy import Column, Integer, String, ForeignKey, DECIMAL, BigInteger
              from botocore.exceptions import ClientError
              from concurrent.futures import ThreadPoolExecutor
              import time
              from itertools import islice
              </span>

              <span style="color: rgb(0, 255, 0);">##################################### Account Lookup Dictionary #################################################</span>

              account_lookup = [
                  {
                      'sdlc': 'prod',
                      'account_id': '551796573889',
                      'region': 'us-east-1',
                      'role_arn': 'arn:aws:iam::551796573889:role/jenkinsAdminXacnt'
                  },
                  {
                      'sdlc': 'dev',
                      'account_id': '061039789243',
                      'region': 'us-east-1',
                      'role_arn': 'arn:aws:iam::061039789243:role/jenkinsAdminXacnt'
                  }
              ]

              <span style="color: rgb(0, 255, 0);">##################################### Database Class Definitions ################################################</span>

              Base = declarative_base()
              class S3BUCKETS(Base):
                  __tablename__ = 's3'
                  id = Column(Integer, primary_key=True)
                  account_id = Column(String)
                  bucket = Column(String)
                  totalSizeBytes = Column(BigInteger)
                  totalSizeKb = Column(DECIMAL)
                  totalSizeMb = Column(DECIMAL)
                  totalSizeGb = Column(DECIMAL)
                  costPerMonth = Column(DECIMAL)
                  objects = relationship('S3BUCKETOBJECTS', backref='s3objects')
                  def __repr__(self):
                      return f'Bucket {self.bucket}'

              class S3BUCKETOBJECTS(Base):
                  __tablename__ = 's3objects'
                  id = Column(Integer, primary_key=True)
                  bucket_id = Column(Integer, ForeignKey('s3.id'))
                  bucket = Column(String)
                  key = Column(String)
                  sizeBytes = Column(BigInteger)
                  sizeKb = Column(DECIMAL)
                  sizeMb = Column(DECIMAL)
                  sizeGb = Column(DECIMAL)
                  costPerMonth = Column(DECIMAL)

              <span style="color: rgb(0, 255, 0);">##################################### Lambda Handler ############################################################</span>

              def lambda_handler(event, context):
                  logger.info("Starting backfill process...")
                  start_time = time.time()

                  for account in account_lookup:


                      logger.info("Retrieving existing buckets for {}...", account['account_id'])

                      session = getAccountSession(account)

                      buckets = getBucketsData(session)

                      logger.info("Backfilling database for {}...", account['account_id'])

                      backfillDatabase(buckets, account['account_id'])
                  
                  end_time = time.time()

                  logger.info(f"Execution time: {end_time - start_time} seconds")

              <span style="color: rgb(0, 255, 0);">##################################### Main Backfill Function ####################################################</span>

              def backfillDatabase(buckets: List[dict], account_id: str) -> dict:
                  try:
                      current_keys = set()
                      objects_to_add = []
                      session = getDatabaseSession()
                      existing_db_buckets = getExistingDbBuckets(session, account_id)
                      existing_db_objects = getExistingDbObjects(session, account_id)

                      logger.info('Processing any new buckets and objects...')
                      with ThreadPoolExecutor() as executor:
                          futures = [
                              executor.submit(processBucket, bucket, existing_db_buckets, existing_db_objects, account_id, current_keys, objects_to_add) for bucket in buckets
                          ]
                          for future in futures:
                              try:
                                  future.result()
                              except Exception as e:
                                  logger.error(f'Error processing bucket: {str(e)}')

                      current_buckets = {bucket['bucket'] for bucket in buckets}

                      logger.info('Buckets processed.')

                      logger.info('Checking for objects to add...')

                      if objects_to_add:
                          logger.info('Adding new objects')
                          logger.info(f'Batching {len(objects_to_add)} objects to add to the database...')
                          batch_size = 100
                          logger.info(f'Batch size: {batch_size}')
                          objects_to_add_batches = chunked_iterable([obj for obj in objects_to_add], batch_size)
                          with ThreadPoolExecutor() as executor:
                              futures = [
                                  executor.submit(addObjects, batch, count)
                                  for count, batch in enumerate(objects_to_add_batches)
                              ]
                              for future in futures:
                                  try:
                                      future.result()
                                  except Exception as e:
                                      logger.error(f'Error adding objects: {str(e)}')

                      logger.info('Checking for objects to delete...')
                      objects_to_delete = set(existing_db_objects.keys()) - current_keys

                      if objects_to_delete:
                          logger.info(f'Batching {len(objects_to_delete)} objects for deletion')
                          batch_size = 100
                          logger.info(f'Batch size: {batch_size}')
                          logger.info(f'Number of batches: {len(objects_to_delete) // batch_size}')
                          objects_to_delete_batches = chunked_iterable([existing_db_objects[obj]['id'] for obj in objects_to_delete], batch_size)
                          with ThreadPoolExecutor() as executor:
                              futures = [
                                  executor.submit(deleteObjects, batch, count)
                                  for count, batch in enumerate(objects_to_delete_batches)
                              ]
                              for future in futures:
                                  try:
                                      future.result()
                                  except Exception as e:
                                      logger.error(f'Error deleting objects: {str(e)}')
                      
                      logger.info('Checking for buckets to delete...')
                      buckets_to_delete = set(existing_db_buckets.keys()) - current_buckets
                      if buckets_to_delete:
                          for bucket in buckets_to_delete:
                              try:
                                  logger.info(f'Deleting bucket from database {bucket}')
                                  deleteBucket(session, existing_db_buckets[bucket])
                              except Exception as e:
                                  logger.error(f'Error deleting bucket: {str(e)}')

                  except Exception as e:
                      session.rollback()
                      return {
                          'statusCode': 500,
                          'body': json.dumps(f'An error occurred: {e}')
                      }
                  finally:
                      session.commit()
                      session.close()


              <span style="color: rgb(0, 255, 0);">##################################### Bucket Processing Function ################################################</span>

              def processBucket(bucket: Dict, existing_db_buckets: Dict, existing_db_objects: Dict, account_id: str, current_keys: set, objects_to_add: list[S3BUCKETOBJECTS]) -> None:
                  bucket_id = existing_db_buckets.get(bucket['bucket'])

                  thread_session = getThreadsafeDatabaseSession()
                  if bucket_id:
                      if bucketNeedsUpdate(thread_session, bucket_id, bucket, account_id):
                          modifyBucket(thread_session, bucket_id, bucket, account_id)
                          logger.info(f'Bucket {bucket["bucket"]} updated.')
                  else:
                      logger.info(f'Adding bucket {bucket["bucket"]} to the database.')
                      bucket_id = addBucket(thread_session, bucket, account_id)
                  for obj in bucket['objects']:
                      object_key = (bucket_id, obj['key'])
                      if object_key in existing_db_objects:
                          existing_object = existing_db_objects[object_key]
                          if objectNeedsUpdate(existing_object, obj):
                              modifyObject(thread_session, existing_object['id'], obj)
                      else:
                          objects_to_add.append({
                              'bucket_id': bucket_id,
                              'obj': obj
                          })

                      current_keys |= {object_key}

                  logger.info(f'Existing object hashes in {bucket["bucket"]} checked.')

                  thread_session.commit()
                  thread_session.remove()


              <span style="color: rgb(0, 255, 0);">#################################### Helper Functions for Buckets ##################################################</span>

              def bucketNeedsUpdate(session, bucket_id: int, bucket: Dict, account_id: str) -> bool:
                  existing_bucket = session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).one()
                  return (
                      existing_bucket.totalSizeBytes != bucket['totalSizeBytes']
                  )

              def modifyBucket(session, bucket_id: int, bucket: Dict, account_id: str) -> None:
                  session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).update({
                      "account_id": account_id,
                      "totalSizeBytes": bucket["totalSizeBytes"],
                      "totalSizeKb": bucket["totalSizeKb"],
                      "totalSizeMb": bucket["totalSizeMb"],
                      "totalSizeGb": bucket["totalSizeGb"],
                      "costPerMonth": bucket["costPerMonth"],
                  })
                  session.commit()


              def addBucket(session: sessionmaker, bucket: Dict, account_id: str) -> str:
                  new_bucket = S3BUCKETS(
                      account_id=account_id,
                      bucket=bucket['bucket'],
                      totalSizeBytes=bucket['totalSizeBytes'],
                      totalSizeKb=bucket['totalSizeKb'],
                      totalSizeMb=bucket['totalSizeMb'],
                      totalSizeGb=bucket['totalSizeGb'],
                      costPerMonth=bucket['costPerMonth']
                  )
                  session.add(new_bucket)
                  session.commit()
                  return new_bucket.id

              def deleteBucket(session: sessionmaker, bucket_id: int) -> None:
                  session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.bucket_id == bucket_id).delete()
                  session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).delete()
                  session.commit()

              <span style="color: rgb(0, 255, 0);">##################################### Helper Functions for Objects #################################################</span>

              def objectNeedsUpdate(existing_object, object: Dict) -> bool:
                  return (
                      existing_object['hash'] != object['hash']
                  )

              def modifyObject(session: sessionmaker, object_id: int, object: Dict) -> None:
                  session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.id == object_id).update({
                      "sizeBytes": object["sizeBytes"],
                      "sizeKb": object["sizeKb"],
                      "sizeMb": object["sizeMb"],
                      "sizeGb": object["sizeGb"],
                      "costPerMonth": object["costPerMonth"]
                  })
                  session.commit()

              def addObjects(batch: list[S3BUCKETOBJECTS], count: int) -> None:
                  thread_session = getThreadsafeDatabaseSession()
                  logger.info(f'Adding batch #{count} to the database...')
                  for obj in batch:
                      # logger.info(f'Adding object {obj["obj"]["key"]} to the database.')
                      new_object = S3BUCKETOBJECTS(
                          bucket_id=obj['bucket_id'],
                          bucket=obj['obj']['bucket'],
                          key=obj['obj']['key'],
                          sizeBytes=obj['obj']['sizeBytes'],
                          sizeKb=obj['obj']['sizeKb'],
                          sizeMb=obj['obj']['sizeMb'],
                          sizeGb=obj['obj']['sizeGb'],
                          costPerMonth=obj['obj']['costPerMonth']
                      )
                      thread_session.add(new_object)
                  thread_session.commit()
                  thread_session.remove()
                  logger.info(f'Batch #{count} added to the database.')

              def deleteObjects(object_ids: list[int], count: int) -> None:
                  thread_session = getThreadsafeDatabaseSession()
                  logger.info(f'Deleting batch #{count}')
                  thread_session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.id.in_(object_ids)).delete(synchronize_session='fetch')
                  logger.info(f'Batch #{count} deleted.')
                  thread_session.commit()
                  thread_session.remove()

              def chunked_iterable(iterable, size):
                  iterator = iter(iterable)
                  for first in iterator:
                      yield [first] + list(islice(iterator, size - 1))


              <span style="color: rgb(0, 255, 0);">##################################### AWS Helper Functions #################################################</span>

              def getAccountSession(account: dict) -> boto3.session.Session:
                  session = boto3.Session()
                  sts = session.client('sts')
                  response = sts.assume_role(
                      RoleArn=account['role_arn'],
                      RoleSessionName='s3-backfill',
                      DurationSeconds=900
                  )
                  credentials = response['Credentials']
                  account_session = boto3.Session(
                      aws_access_key_id=credentials['AccessKeyId'],
                      aws_secret_access_key=credentials['SecretAccessKey'],
                      aws_session_token=credentials['SessionToken'],
                      region_name=account['region']
                  )
                  return account_session

              def getBucketsData(session: boto3.session.Session) -> list:
                  logger.info('Retrieving S3 bucket data from AWS...')
                  bucket_list = []

                  s3 = session.client('s3')
                  buckets = s3.list_buckets()

                  for bucket in buckets['Buckets']:
                      bucket_dict = {
                          'bucket': bucket['Name'],
                          'totalSizeBytes': 0,
                          'totalSizeKb': 0,
                          'totalSizeMb': 0,
                          'totalSizeGb': 0,
                          'costPerMonth': 0,
                          'objects': []
                      }
                      
                      paginator = s3.get_paginator('list_objects_v2')
                  
                      object_list = []
                  
                      total_bucket_cost = 0
                  
                      object_iterator = paginator.paginate(Bucket=bucket['Name'])
                      for page in object_iterator:
                          if 'Contents' in page:
                              for obj in page['Contents']:
                                  object_dict = {
                                      'key': obj['Key'],
                                      'bucket': bucket['Name'],
                                      'sizeBytes': obj['Size'],
                                      'sizeKb': round(obj['Size'] / 1024, 2),
                                      'sizeMb': round(obj['Size'] / (1024 * 1024), 2),
                                      'sizeGb': round(obj['Size'] / (1024 * 1024 * 1024), 4),
                                  }
                                  object_dict['costPerMonth'] = object_dict['sizeGb'] * 0.023
                                  object_dict['hash'] = hash((object_dict['sizeBytes'], object_dict['costPerMonth']))
                                  total_bucket_cost = total_bucket_cost + object_dict['costPerMonth']
                                  object_list.append(object_dict)

                      bucket_dict['totalSizeBytes'] = sum([obj['sizeBytes'] for obj in object_list])
                      bucket_dict['totalSizeKb'] = round(sum([obj['sizeKb'] for obj in object_list]), 4)
                      bucket_dict['totalSizeMb'] = round(sum([obj['sizeMb'] for obj in object_list]), 4)
                      bucket_dict['totalSizeGb'] = round(sum([obj['sizeGb'] for obj in object_list]), 4)
                      bucket_dict['costPerMonth'] = round(total_bucket_cost, 6)
                      bucket_dict['objects'] = object_list
                      bucket_list.append(bucket_dict)

                  logger.info('S3 bucket data retrieved.')
                  return bucket_list

              def getDatabaseCredentials() -> dict:
                  secret_id = "arn:aws:secretsmanager:us-east-1:061039789243:secret:rds!db-555390f8-60f2-4d37-ad75-e63d8f0cbfa9-0s9oyX"
                  region = "us-east-1"
                  session = boto3.session.Session()
                  client = session.client('secretsmanager', region_name=region)

                  try:
                      secret_response = client.get_secret_value(SecretId=secret_id)
                      secret = secret_response['SecretString']
                      json_secret = json.loads(secret)
                      credentials = {
                          'username': json_secret['username'],
                          'password': json_secret['password']
                      }
                      return credentials
                  except ClientError as e:
                      raise e

              <span style="color: rgb(0, 255, 0);">##################################### Database Helper Functions #################################################</span>

              def getEngine() -> create_engine:
                  credentials = getDatabaseCredentials()
                  engine = create_engine(
                      f'postgresql://{credentials["username"]}:{credentials["password"]}@resources.czmo2wqo0w7e.us-east-1.rds.amazonaws.com:5432'
                  )
                  return engine

              def getDatabaseSession() -> sessionmaker:
                  engine = getEngine()
                  Session = sessionmaker(bind=engine)
                  session = Session()
                  return session

              def getThreadsafeDatabaseSession() -> scoped_session:
                  engine = getEngine()
                  session_factory = sessionmaker(bind=engine)
                  session = scoped_session(session_factory)
                  return session

              def getExistingDbBuckets(session: sessionmaker, account_id: str) -> set:
                  buckets = {b.bucket: b.id for b in session.query(S3BUCKETS).filter(S3BUCKETS.account_id == account_id).all()}
                  return buckets

              def getExistingDbObjects(session: sessionmaker, account_id: int) -> set:
                  objects = {
                      (obj.bucket_id, obj.key): {
                          "id": obj.id, 
                          "hash": hash((obj.sizeBytes, obj.costPerMonth))
                      } 
                      for obj in session.query(S3BUCKETOBJECTS).join(S3BUCKETS, S3BUCKETS.id == S3BUCKETOBJECTS.bucket_id).filter(
                          S3BUCKETS.account_id == account_id
                      ).all()
                  }
                  return objects
                  <br><br>

        - content: |
              I am using a combination of boto3 and SQLAlchemy ORM (Object Relational Mapping) for the bulk of the backfill operations. The threading libraries are built in to Python as well as the itertools used to manage my batch sizes when processing object data chunks. To run in Lambda, AWS-specific psychopg libraries are available to support PostgreSQL dialect in SQLAlchemy. Additionally, I prefer Loguru for logging so I need to package the following imports for my function. <br><br>

              <span style="opacity: 50%">(boto3 is optional here as Lambda functions have this library pre-installed. But to be safe and ensure fine-grained control over my dependencies, I include it for the sake of completeness.)</span>
              <br><br>

          imgs:
            - /assets/project-images/backfill-images/backfill1_1.png

    - title: Structure of the Code
      tabTitle: Structure
      subsections:
        - content: |
              The code is structured into several key components that work together to comprise the backfill process:

        - content:

          listItems:
            - text: "<span style='color: blue'>Account Lookup Dictionary:</span> Contains account information for role assumption and region selection."
          code: |
              <br>##################################### <span style='color: rgb(172, 172, 255)'>Account Lookup Dictionary</span> #################################################

              <span style="color: rgb(0, 255, 0);">account_lookup</span> = [
                {
                    'sdlc': 'prod',
                    'account_id': '551796573889',
                    'region': 'us-east-1',
                    'role_arn': 'arn:aws:iam::551796573889:role/jenkinsAdminXacnt'
                },
                ...
                <br><br>

        - content:

          listItems:
            - text: "<span style='color: blue'>Database Class Definitions:</span> Defines the database schema for S3 buckets and objects."
          code: |
              <br>##################################### <span style='color: rgb(172, 172, 255)'>Database Class Definitions</span> ################################################

              Base = declarative_base()
              <span style="color: rgb(0, 255, 0);">class S3BUCKETS</span>(Base):
                  __tablename__ = 's3'
                  id = Column(Integer, primary_key=True)
                  account_id = Column(String)
                  bucket = Column(String)
                  totalSizeBytes = Column(BigInteger)
                  totalSizeKb = Column(DECIMAL)
                  totalSizeMb = Column(DECIMAL)
              ...
              <br><br>

        - content:

          listItems:
            - text: "<span style='color: blue'>Lambda Handler:</span> Entry point for the Lambda function, retrieves account data and initiates the backfill process."
          code: |
              <br>##################################### <span style='color: rgb(172, 172, 255)'>Lambda Handler</span> ############################################################


              <span style="color: rgb(0, 255, 0);">def lambda_handler</span>(event, context):
                  logger.info("Starting backfill process...")
                  start_time = time.time()

                  for account in account_lookup:


                      logger.info("Retrieving existing buckets for {}...", account['account_id'])

                      session = getAccountSession(account)
              ...
              <br><br>

        - content:

          listItems:
            - text: "<span style='color: blue'>Backfill Logic:</span> Orchestrates the backfill process, including bucket and object processing."
          code: |
              <br>##################################### <span style='color: rgb(172, 172, 255)'>Main Backfill Function</span> ####################################################


              <span style="color: rgb(0, 255, 0);">def backfillDatabase</span>(buckets: List[dict], account_id: str) -> dict:
                  try:
                      current_keys = set()
                      objects_to_add = []
                      session = getDatabaseSession()
              ...
              <br><br>

        - content:

          listItems:
            - text: "<span style='color: blue'>Bucket Processing Function:</span> Handles bucket operations, including updates and deletions."
          code: |
              <br>##################################### <span style='color: rgb(172, 172, 255)'>Bucket Processing Function</span> ################################################


              <span style="color: rgb(0, 255, 0);">def processBucket</span>(bucket: Dict, existing_db_buckets: Dict, existing_db_objects: Dict, account_id: str, current_keys: set, objects_to_add: list[S3BUCKETOBJECTS]) -> None:
                  bucket_id = existing_db_buckets.get(bucket['bucket'])
                  try:
                      thread_session = getThreadsafeDatabaseSession()
                      if bucket_id:
                          if bucketNeedsUpdate(thread_session, bucket_id, bucket, account_id):
              ...
              <br><br>

        - content:

          listItems:
            - text: "<span style='color: blue'>Helper Functions:</span> Provides additional functionality for database operations, object comparisons, and AWS interactions."
          code: |
              <br>#################################### <span style='color: rgb(172, 172, 255)'>Helper Functions for Buckets</span> ##################################################


              <span style="color: rgb(0, 255, 0);">def bucketNeedsUpdate</span>(session, bucket_id: int, bucket: Dict, account_id: str) -> bool:
                  existing_bucket = session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).one()
              ...
              <span style="color: rgb(0, 255, 0);">def modifyBucket</span>(session, bucket_id: int, bucket: Dict, account_id: str) -> None:
                  session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).update({
              ...
              <span style="color: rgb(0, 255, 0);">def addBucket</span>(session: sessionmaker, bucket: Dict, account_id: str) -> str:
                  new_bucket = S3BUCKETS(
              ...
              <span style="color: rgb(0, 255, 0);">def deleteBucket</span>(session: sessionmaker, bucket_id: int) -> None:
                  session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.bucket_id == bucket_id).delete()
              ...

              <br>##################################### <span style='color: rgb(172, 172, 255)'>Helper Functions for Objects</span> #################################################

              <span style="color: rgb(0, 255, 0);">def objectNeedsUpdate</span>(existing_object, object: Dict) -> bool:
                  return (
              ...
              <span style="color: rgb(0, 255, 0);">def modifyObject</span>(session: sessionmaker, object_id: int, object: Dict) -> None:
                  session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.id == object_id).update({
              ...
              <span style="color: rgb(0, 255, 0);">def addObjects</span>(batch: list[S3BUCKETOBJECTS], count: int) -> None:
                  thread_session = getThreadsafeDatabaseSession()
              ...
              <span style="color: rgb(0, 255, 0);">def deleteObjects</span>(object_ids: list[int], count: int) -> None:
                  thread_session = getThreadsafeDatabaseSession()
              ...
              <span style="color: rgb(0, 255, 0);">def chunked_iterable</span>(iterable, size):
                  iterator = iter(iterable)
              ...

              <br>##################################### <span style='color: rgb(172, 172, 255)'>AWS Helper Functions</span> #################################################
              </span>
              <span style="color: rgb(0, 255, 0);">def getAccountSession</span>(account: dict) -> boto3.session.Session:
                  session = boto3.Session()
              ...
              <span style="color: rgb(0, 255, 0);">def getBucketsData</span>(session: boto3.session.Session) -> list:
                  logger.info('Retrieving S3 bucket data from AWS...')
              ...
              <span style="color: rgb(0, 255, 0);">def getDatabaseCredentials</span>() -> dict:
                  secret_id = "arn:aws:secretsmanager:us-east-1:061039789243:secret:rds!db-555390f8-60f2-4d37-ad75-e63d8f0cbfa9-0s9oyX"
              ...

              <br>##################################### <span style='color: rgb(172, 172, 255)'>Database Helper Functions</span> #################################################

              <span style="color: rgb(0, 255, 0);">def getEngine</span>() -> create_engine:
                  credentials = getDatabaseCredentials()
              ...
              <span style="color: rgb(0, 255, 0);">def getDatabaseSession</span>() -> sessionmaker:
                  engine = getEngine()
              ...
              <span style="color: rgb(0, 255, 0);">def getThreadsafeDatabaseSession</span>() -> scoped_session:
                  engine = getEngine()
              ...
              <span style="color: rgb(0, 255, 0);">def getExistingDbBuckets</span>(session: sessionmaker, account_id: str) -> set:
                  buckets = {b.bucket: b.id for b in session.query(S3BUCKETS).filter(S3BUCKETS.account_id == account_id).all()}
              ...
              <span style="color: rgb(0, 255, 0);">def getExistingDbObjects</span>(session: sessionmaker, account_id: int) -> set:
                  objects = {
              ...
              <br><br>

    - title: Account Lookup and Database Schema
      tabTitle: Acct/Schema
      subsections:
        - content: |
              The account lookup dictionary contains information to access each of my accounts, including the environment (sdlc), account ID, and region and role ARN needed to connect using boto3. This information will be used to gather the extant bucket data from all of my accounts, and accounts can be added in the future as needed using this dictionary schema. As long as I have the necessary permissions setup on the created role, I can access the account data and process it accordingly.<br><br>
          code: |
              <br>account_lookup = [
                  {
                      '<span style="color: rgb(0, 255, 0);">sdlc</span>': 'prod', <span style="color: rgb(0, 255, 0);"># Account Environment</span>
                      '<span style="color: rgb(0, 255, 0);">account_id</span>': '551796573889',
                      '<span style="color: rgb(0, 255, 0);">region</span>': 'us-east-1',
                      '<span style="color: rgb(0, 255, 0);">role_arn</span>': 'arn:aws:iam::551796573889:role/jenkinsAdminXacnt'
                  },
                  {
                      'sdlc': 'dev',
                      'account_id': '061039789243',
                      'region': 'us-east-1',
                      'role_arn': 'arn:aws:iam::061039789243:role/jenkinsAdminXacnt'
                  }
              ]<br>

        - content: |
              The database schema classes are defined using SQLAlchemy ORM to simplify the process of defining and managing the database structure in a PostgreSQL database.<br><br>
              The <span style="color: blue;">declarative_base()</span> initializes a base class for SQLAlchemy models, containing the metadata and functionality needed to successfully map a Python class to a database table.<br><br>

              The <strong style="color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;">S3BUCKETS</strong> schema represents buckets and includes a <span style="color: blue;">backref</span> relationship to the <strong style="color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;">S3BUCKETOBJECTS</strong> schema, enabling seamless access to all objects associated with a specific bucket.<br><br>

              The <strong style="color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;">S3BUCKETOBJECTS</strong> schema represents individual objects within buckets and includes a <span style="color: blue;">ForeignKey</span> relationship linking each object to its corresponding bucket in the <strong style="color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;">S3BUCKETS</strong> schema. This relationship allows for efficient querying of the bucket associated with any given object.<br><br>

              Together, this schema structure efficiently stores and organizes the bucket and object data gathered from AWS accounts in the database, enabling easy querying and updates.<br><br>

          code: |
              <br><span style='color: red'>Base</span> = declarative_base()
              <span style='color: rgb(0, 255, 0)'>class</span> <span style='color: rgb(172, 172, 255)'>S3BUCKETS</span>(<span style='color: red'>Base</span>):
                  __tablename__ = '<span style='color: orange'>s3</span>'
                  <span style='color: rgb(0, 255, 0)'>id</span> = Column(Integer, primary_key=True)
                  <span style='color: rgb(0, 255, 0)'>account_id</span> = Column(String)
                  <span style='color: rgb(0, 255, 0)'>bucket</span> = Column(String)
                  <span style='color: rgb(0, 255, 0)'>totalSizeBytes</span> = Column(BigInteger)
                  <span style='color: rgb(0, 255, 0)'>totalSizeKb</span> = Column(DECIMAL)
                  <span style='color: rgb(0, 255, 0)'>totalSizeMb</span> = Column(DECIMAL)
                  <span style='color: rgb(0, 255, 0)'>totalSizeGb</span> = Column(DECIMAL)
                  <span style='color: rgb(0, 255, 0)'>costPerMonth</span> = Column(DECIMAL)
                  <span style='color: rgb(0, 255, 0)'>objects</span> = relationship('<span style='color: rgb(172, 172, 255)'>S3BUCKETOBJECTS</span>', <span style='color: rgb(172, 172, 255)'>backref</span>='<span style='color: orange'>s3objects</span>')
                  def __repr__(self):
                      return f'Bucket {self.bucket}'

              <span style='color: rgb(0, 255, 0)'>class</span> <span style='color: rgb(172, 172, 255)'>S3BUCKETOBJECTS</span>(Base):
                  __tablename__ = '<span style='color: orange'>s3objects</span>'
                  <span style='color: rgb(0, 255, 0)'>id</span> = Column(Integer, primary_key=True)
                  <span style='color: rgb(0, 255, 0)'>bucket_id</span> = Column(Integer, <span style='color: rgb(172, 172, 255)'>ForeignKey</span>('<span style='color: orange'>s3</span>.<span style='color: rgb(0, 255, 0)'>id</span>'))
                  <span style='color: rgb(0, 255, 0)'>bucket</span> = Column(String)
                  <span style='color: rgb(0, 255, 0)'>key</span> = Column(String)
                  <span style='color: rgb(0, 255, 0)'>sizeBytes</span> = Column(BigInteger)
                  <span style='color: rgb(0, 255, 0)'>sizeKb</span> = Column(DECIMAL)
                  <span style='color: rgb(0, 255, 0)'>sizeMb</span> = Column(DECIMAL)
                  <span style='color: rgb(0, 255, 0)'>sizeGb</span> = Column(DECIMAL)
                  <span style='color: rgb(0, 255, 0)'>costPerMonth</span> = Column(DECIMAL)<br>

    - title: Lambda Handler Function
      tabTitle: Lambda Handler
      subsections:
        - content: |
              The Lambda handler function serves as the entry point for the backfill process. It retrieves the account data from the account lookup dictionary, initiates the backfill process for each account, and logs the execution time upon completion.<br><br>

              Typically the Lambda handler function is triggered by an event, such as an SQS message or EventBridge schedule, but here it is manually invoked or I can simply trigger it using a cron job or other scheduling mechanism with an empty event payload.<br><br>

              The handler function iterates over each account from my account dictionary and performs the following steps:
          listItems:
            - text: "Retrieves a session for the account using the role ARN and region."
            - text: "Retrieves all bucket data for the account using the session."
            - text: "Initiates the backfill process for each account using the account bucket data as a parameter sent to the backfill function."
          code: |
              <br><span style="color: rgb(0, 255, 0);">def lambda_handler</span>(event, context):
                  logger.info("Starting backfill process...")
                  start_time = time.time()

                  for account in account_lookup:


                      logger.info("Retrieving existing buckets for {}...", account['account_id'])

                      session = <span style="color: rgb(0, 255, 0);">getAccountSession</span>(account)

                      buckets = <span style="color: rgb(0, 255, 0);">getBucketsData</span>(session)

                      logger.info("Backfilling database for {}...", account['account_id'])

                      <span style="color: rgb(0, 255, 0);">backfillDatabase</span>(buckets, account['account_id'])

                  end_time = time.time()

                  logger.info(f"Execution time: {end_time - start_time} seconds")<br>
        - content: |
              The execution time is captured as a benchmark for the sake of monitoring the effects of each optimization I added to the overall process. I was able to bring the entire process down from over an hour to minutes using the combination of optimizations I mentioned and will go over in detail in future sections.<br><br>

    - title: AWS Helper Functions for Sessions and Data Retrieval
      tabTitle: AWS Functions
      subsections:
        - content: |
              Before performing comparisons and backfilling the database, I retrieve the latest bucket data from all AWS accounts and store it in memory. The following AWS helper functions are used to fetch the necessary data:
          listItems:
            - text: "<span style='color: blue;'>getAccountSession</span>"
              subList:
                - "Retrieves a session for the specified account using the role ARN and region. It assumes the specified role and returns a session object that can be used to interact with the account's resources."
            - text: "<span style='color: blue;'>getBucketsData</span>"
              subList:
                - "Retrieves all bucket data for the specified account using the provided session. It retrieves the list of buckets and their associated objects, calculates the total size and cost for each bucket, and returns the bucket data as a list of dictionaries."
            - text: "<span style='color: blue;'>getDatabaseCredentials</span>"
              subList:
                - "Retrieves the database credentials from AWS Secrets Manager. The credentials are used to establish a connection to the PostgreSQL database."
        - content: |
              The first function simply helps me to retrieve a <span style='color: blue;'>boto3 session</span> object for the specified account using the role ARN and region. This function is used to interact with the account's resources and retrieve the necessary data for the backfill process. This assumes that the role (or user) running this script has the necessary permissions to assume the role defined in the account lookup dictionary. In this case I am assuming my jenkinsXacntRole from another project that happens to have the required permissions to both be assumed by the role I am using and to perform all of the AWS operations in the script, including secret retrieval and S3 bucket data retrieval.<br><br>
          code: |
              <br>
              <span style="color: rgb(0, 255, 0);">def getAccountSession</span>(account: dict) -> <span style="color: orange;">boto3.session.Session</span>:
                  <span style="color: orange;">session</span> = <span style="color: orange;">boto3.Session()</span>
                  <span style="color: red;">sts</span> = <span style="color: orange;">session</span>.client('sts')
                  <span style="color: rgb(172, 172, 255);">response</span> = <span style="color: red;">sts</span>.assume_role(
                      RoleArn=account['role_arn'],
                      RoleSessionName='s3-backfill',
                      DurationSeconds=900
                  )
                  <span style="color: rgb(172, 172, 255);">credentials</span> = response['Credentials']
                  <span style="color: orange;">account_session</span> = boto3.Session(
                      aws_access_key_id=<span style="color: rgb(172, 172, 255);">credentials</span>['AccessKeyId'],
                      aws_secret_access_key=<span style="color: rgb(172, 172, 255);">credentials</span>['SecretAccessKey'],
                      aws_session_token=<span style="color: rgb(172, 172, 255);">credentials</span>['SessionToken'],
                      region_name=account['region']
                  )
                  return <span style="color: orange;">account_session</span>
              <br><br>
          imgs:
            - /assets/project-images/backfill-images/backfill5_1.png
            - /assets/project-images/backfill-images/backfill5_2.png
        - content: |
              For each account, the second helper function uses the boto3 session to retrieve all of the current AWS bucket data. This function retrieves the list of buckets and their associated objects, calculates the total size and cost for each bucket, and returns the bucket data as a list of dictionaries. This data is then used as a parameter for the backfill process to update the database with the latest bucket information.<br><br>

              The goal of collecting this data was to create a 
          code: |
              <br>
              def getBucketsData(session: boto3.session.Session) -> list:
                  logger.info('Retrieving S3 bucket data from AWS...')
                  bucket_list = []

                  s3 = session.client('s3')
                  buckets = s3.list_buckets()

                  for bucket in buckets['Buckets']:
                      bucket_dict = {
                          'bucket': bucket['Name'],
                          'totalSizeBytes': 0,
                          'totalSizeKb': 0,
                          'totalSizeMb': 0,
                          'totalSizeGb': 0,
                          'costPerMonth': 0,
                          'objects': []
                      }
                      
                      paginator = s3.get_paginator('list_objects_v2')
                  
                      object_list = []
                  
                      total_bucket_cost = 0
                  
                      object_iterator = paginator.paginate(Bucket=bucket['Name'])
                      for page in object_iterator:
                          if 'Contents' in page:
                              for obj in page['Contents']:
                                  object_dict = {
                                      'key': obj['Key'],
                                      'bucket': bucket['Name'],
                                      'sizeBytes': obj['Size'],
                                      'sizeKb': round(obj['Size'] / 1024, 2),
                                      'sizeMb': round(obj['Size'] / (1024 * 1024), 2),
                                      'sizeGb': round(obj['Size'] / (1024 * 1024 * 1024), 4),
                                  }
                                  object_dict['costPerMonth'] = object_dict['sizeGb'] * 0.023
                                  object_dict['hash'] = hash((object_dict['sizeBytes'], object_dict['costPerMonth']))
                                  total_bucket_cost = total_bucket_cost + object_dict['costPerMonth']
                                  object_list.append(object_dict)

                      bucket_dict['totalSizeBytes'] = sum([obj['sizeBytes'] for obj in object_list])
                      bucket_dict['totalSizeKb'] = round(sum([obj['sizeKb'] for obj in object_list]), 4)
                      bucket_dict['totalSizeMb'] = round(sum([obj['sizeMb'] for obj in object_list]), 4)
                      bucket_dict['totalSizeGb'] = round(sum([obj['sizeGb'] for obj in object_list]), 4)
                      bucket_dict['costPerMonth'] = round(total_bucket_cost, 6)
                      bucket_dict['objects'] = object_list
                      bucket_list.append(bucket_dict)

                  logger.info('S3 bucket data retrieved.')
                  return bucket_list
              <br><br>

    - title: Backfill Main Function Logic
      tabTitle: Backfilling
      subsections:
        - content: |
              Now that all of the historical bucket data has been retrieved from AWS, the main backfill function is called and orchestrates the backfill process for each account. This acts as the central hub for processing the existing bucket data and managing any required database updates. At a high level, the backfill performs the following steps:
          listItems:
            - text: "Receives existing bucket data from AWS as a parameter."
            - text: "Session is initialized and existing database buckets and objects are retrieved."
            - text: "Buckets are processed in parallel using a <span style='color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;'>ThreadPoolExecutor</span>. Processing includes:"
              subList:
                - "<span style='color: blue;'>Adding</span> new buckets to the database."
                - "<span style='color: blue;'>Modifying</span> existing bucket metadata as needed."
                - "<span style='color: blue;'>Queueing</span> objects to be added to the database."
                - "<span style='color: blue;'>Modifying</span> existing objects metadata as needed."
                - "<span style='color: blue;'>Incrementing</span> set of current object keys from AWS bucket data."
            - text: "Adds queued new objects to the database in <span style='color: blue;'>batches</span> using a <span style='color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;'>ThreadPoolExecutor</span>."
            - text: "Checks for stale objects to be deleted from the database with a <span style='color: blue;'>set comparison</span>."
            - text: "Deletes stale objects in <span style='color: blue;'>batches</span> using <span style='color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;'>ThreadPoolExecutor</span>"
            - text: "Checks for stale buckets in the database with a set comparison."
            - text: "Deletes stale buckets from the database."
          imgs:
            - /assets/project-images/backfill-images/backfill_function_diagram.png
        - content: |
              The function first initializes several data objects to store information to be used in comparison with existing bucket data in AWS. The first of these is a set to store the keys of current objects in AWS as the buckets are processed. This is incrementally updated as each bucket is processed and using a Python set eliminates the need for duplcate checking.<br><br>

              A list is then initialized that will be incrementally updated with objects that will need to be added to the database later in the process. This happens inside of the <span style="color: green;">processBucket</span> function. This list will then be broken up into batches and added to the database in parallel.<br><br>

          #   A database session object is retrived using a helper function to retrieve the existing buckets and objects from the database, stored in the following format:<br><br>

          #   <span style="color: blue;">existing_db_buckets</span> = {bucket_name: bucket_id}<br>
          #   <span style="color: blue;">existing_db_objects</span> = {(bucket_id, object_key): {'id': object_id, 'hash': hash}}<br><br>

          #   There are several reasons for using this dictionary structure for the existing database objects. First, the bucket id and object id are used as a <span style='color: blue'>composite key</span> to allow for fast lookups of objects unique to each bucket, avoiding the possibility of duplicate object names across buckets. More importantly, the object id and <span style='color: blue'>object hash values</span> allow for extremely fast comparisons between objects in AWS and objects in the database as the current objects retrieved from AWS are also assigned a unique hash value that can be compared to the hash value of the existing object in the database. This results in a constant-time complexity (O(1)) for existing object comparison, which is crucial aspect for the performance of the overall process.<br><br>
          code: |
              <br>
              <span style="color: rgb(0, 255, 0);">def backfillDatabase</span>(<span style="color: rgb(172, 172, 255);">buckets: List[dict], account_id: str</span>) -> dict:
                  try:
                    <span style="color: rgb(172, 172, 255);">current_keys</span> = set()
                    <span style="color: rgb(172, 172, 255);">objects_to_add</span> = []
                    <span style="color: rgb(172, 172, 255);">session</span> = <span style="color: rgb(0, 255, 0);">getDatabaseSession</span>()
                    <span style="color: rgb(172, 172, 255);">existing_db_buckets</span> = <span style="color: rgb(0, 255, 0);">getExistingDbBuckets</span>(<span style="color: rgb(172, 172, 255);">session, account_id</span>)
                    <span style="color: rgb(172, 172, 255);">existing_db_objects</span> = <span style="color: rgb(0, 255, 0);">getExistingDbObjects</span>(<span style="color: rgb(172, 172, 255);">session, account_id</span>)
                    <br><br>
        - content: |
              Now that the existing data is stored, the function can now process the data and update the database as needed. The function iterates over each bucket in the list of buckets retrieved from AWS and processes them in parallel using the <span style='color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;'>ThreadPoolExecutor</span>. I will go over this process in depth in the next section.<br><br>

              The current buckets and objects in AWS are stored as seperate <span style="color: blue">sets</span> for efficient comparison with the existing buckets in the database. This process is used for both object and bucket deletion. Using sets is highly efficient because Python's set data structure supports <span style="color: blue">constant-time membership checking</span> and operations like <span style="color: blue">set subtraction</span>.<br><br>

              By comparing the two setsone containing data from AWS and the other from the databasethe differences between them represent the buckets or objects that need to be added to or removed from the database. This approach eliminates the need to iterate over large datasets manually and leverages Python's built-in set operations to save both time and computational resources.<br><br>

              Using set operations in Python, rather than iterating through lists or dictionaries, is an effective way to optimize performance and streamline the process of reconciling AWS and database data.<br><br>
          code: |
              <br>
              logger.info('Processing any new buckets and objects...')
              <span style="color: rgb(0, 255, 0);">
              with ThreadPoolExecutor() as executor:
                  futures = [
                      executor.submit(processBucket, <span style="color: rgb(172, 172, 255);">bucket, existing_db_buckets, existing_db_objects, account_id, current_keys, objects_to_add</span>) <span style="color: rgb(172, 172, 255);">for bucket in buckets</span>
                  ]
                  for future in futures:
                      try:
                          future.result()
                      except Exception as e:
                          logger.error(f'Error processing bucket: {str(e)}')
              </span>
              <span style="color: rgb(172, 172, 255);">
              current_buckets = {bucket['bucket'] for bucket in buckets}
              </span>
              logger.info('Buckets processed.')
              <br><br>
        - content: |
              Now that I have queued up the list of new objects, they can now be batched and added to the database in parallel using the <span style='color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;'>ThreadPoolExecutor</span>. 
              <br><br>
              Here, I am using a helper function to break the database objects into smaller, more manageable chunks for for batch processing. In a future section, I will go over the specifics of how the function returns each generator chunk and why this method is helpful. I have also <span style="color: red">enumerated</span> the batches to keep track the status of each batch as they are processed in parallel.<br><br>
          code: |
              <br>
              logger.info('Checking for objects to add...')

              <span style="color: rgb(172, 172, 255);">if objects_to_add:</span>
                  logger.info('Adding new objects')
                  logger.info(f'Batching {len(objects_to_add)} objects to add to the database...')
                  batch_size = 100
                  logger.info(f'Batch size: {batch_size}')
                  <span style="color: rgb(0, 255, 0);">
                  <span style="color: rgb(172, 172, 255);">objects_to_add_batches =</span> chunked_iterable(<span style="color: rgb(172, 172, 255);">[obj for obj in objects_to_add], batch_size</span>)

                  with ThreadPoolExecutor() as executor:
                      futures = [
                          executor.submit(addObjects, <span style="color: rgb(172, 172, 255);">batch, count</span>)
                          for <span style="color: rgb(172, 172, 255);">count, batch</span> in <span style="color: red;">enumerate</span>(<span style="color: rgb(172, 172, 255);">objects_to_add_batches</span>)
                      ]
                      for future in futures:
                          try:
                              future.result()
                          except Exception as e:
                              logger.error(f'Error adding objects: {str(e)}')
                  </span>
              <br><br>

        - content: |
              After the new objects have been added to the database, I now do some cleanup on any stale objects that may exist in the database. As stated earlier, I am using a <span style='color: blue;'>set subtraction</span> (or set difference) operation to find the objects that need to be deleted from the database. The objects are then similarly batched and processed in parallel using the <span style='color: blue; font-family: monospace, sans-serif; background-color: lightgrey; font-size: small;'>ThreadPoolExecutor</span>.<br><br>
          code: |
              <br>
              logger.info('Checking for objects to delete...')
              <span style="color: rgb(172, 172, 255);">objects_to_delete = <span style="color: orange">set(</span>existing_db_objects.keys()<span style="color: orange">)</span> - current_keys</span>

              <span style="color: rgb(172, 172, 255);">if objects_to_delete:</span>
                  logger.info(f'Batching {len(objects_to_delete)} objects for deletion')
                  batch_size = 100
                  logger.info(f'Batch size: {batch_size}')
                  logger.info(f'Number of batches: {len(objects_to_delete) // batch_size}')
                  <span style="color: rgb(0, 255, 0);">
                  <span style="color: rgb(172, 172, 255);">objects_to_delete_batches =</span> chunked_iterable(<span style="color: rgb(172, 172, 255);">[existing_db_objects[obj]['id'] for obj in objects_to_delete], batch_size</span>)
                  with ThreadPoolExecutor() as executor:
                      futures = [
                          executor.submit(deleteObjects, <span style="color: rgb(172, 172, 255);">batch, count</span>)
                          for <span style="color: rgb(172, 172, 255);">count, batch</span> in <span style="color: red;">enumerate</span>(<span style="color: rgb(172, 172, 255);">objects_to_delete_batches</span>)
                      ]
                      for future in futures:
                          try:
                              future.result()
                          except Exception as e:
                              logger.error(f'Error deleting objects: {str(e)}')
                  </span>
              <br><br> 

        - content: |
              Finally, the buckets can be deleted one by one if there are any stale buckets in the database. The function to delete buckets will also empty all associated objects from the database. Given that this is a less likely scenario and there are far less buckets than objects, this can be done in a single thread without the need for batching. It is possible that there may be a large number of objects to delete from a stale bucket in the database, but typically this can be handled in one operation as these should have been picked up by the automation set up in <a href="/projects/event">my previous project</a>.<br><br>
          code: |
              <br>
                  logger.info('Checking for buckets to delete...')
                  <span style="color: rgb(172, 172, 255);">buckets_to_delete = <span style="color: orange">set(</span>existing_db_buckets.keys()<span style="color: orange">)</span> - current_buckets</span>
                  <span style="color: rgb(0, 255, 0);">
                  <span style="color: rgb(172, 172, 255);">if buckets_to_delete:</span>
                      <span style="color: rgb(172, 172, 255);">for bucket in buckets_to_delete:</span>
                          logger.info(f'Deleting bucket from database {<span style="color: rgb(172, 172, 255);">bucket</span>}')
                          deleteBucket(<span style="color: rgb(172, 172, 255);">session, existing_db_buckets[bucket]</span>)
                  </span>
              except Exception as e:
                  session.rollback()
                  return {
                      'statusCode': 500,
                      'body': json.dumps(f'An error occurred: {e}')
                  }
              finally:
                  session.commit()
                  session.close()
              <br><br> 

    - title: Parallel Bucket Processing Function
      tabTitle: Bucket Processing
      subsections:
        - content: |
              The <span style='color: green;'>processBucket</span> function is called first from within the backfill function. This is the first step for the actual backfilling process and performs several operations on each bucket, including:
          listItems:
            - text: "Comparing AWS buckets with existing database buckets."
              subList:
                - "<span style='color: blue'>Modifying</span> existing bucket metadata."
                - "<span style='color: blue'>Adding</span> new buckets to the database."
            - text: "Comparing AWS objects with existing database objects (using <span style='color: blue'>hashed metadata values</span>)."
              subList:
                - "<span style='color: blue'>Modifying</span> existing object metadata as needed."
                - "<span style='color: blue'>Queuing</span> objects to be added to the database."
            - text: "<span style='color: blue'>Incrementing</span> the set of <span style='color: blue'>current object keys</span> from AWS bucket data."

        - content: |
              The function <span style='color: blue'>queues objects to be added to the database instead of adding them directly</span>. This decoupled approach minimizes direct database writes and improves scalability by batching the additions, which are processed later in parallel for better performance.
              <br><br>
              In contrast, object modifications are handled immediately within the threaded context since they are infrequent and only occur when an object with the same name but different metadata is found. By comparing object hashes in memory, modifications can be quickly identified and applied without needing additional database lookups, ensuring efficient updates with minimal overhead.<br><br>
          imgs:
            - /assets/project-images/backfill-images/backfill_bucketprocess_diagram.png

        - content: ""
          listItems:
            - text: "<span style='color: blue;'>bucket</span>:"
              subList:
                - "The current bucket being processed."
            - text: "<span style='color: blue;'>existing_db_buckets</span>:"
              subList:
                - "A dictionary of existing database buckets."
            - text: "<span style='color: blue;'>existing_db_objects</span>:"
              subList:
                - "A dictionary of existing database objects."
            - text: "<span style='color: blue;'>account_id</span>:"
              subList:
                - "The account ID associated with the bucket."
            - text: "<span style='color: blue;'>current_keys</span>:"
              subList:
                - "A set of current object keys from AWS."
            - text: "<span style='color: blue;'>objects_to_add</span>:"
              subList:
                - "A list of objects to be added to the database."

        - content: |
              The function first checks the database for an existing corresponding bucket ID. Using a <span style='color: blue;'>thread-safe database session</span>, I either update the bucket's metadata if changes are needed or add it as a new entry if it doesnt exist. All actions are logged for monitoring and debugging.<br><br>

              The <span style='color: blue;'>thread-safe database session</span> ensures that multiple threads can access the database concurrently without conflicts or data corruption. This is crucial in a multi-threaded environment, such as when using ThreadPoolExecutor, where multiple buckets might be processed simultaneously. By isolating each threads database operations, the session prevents race conditions (errors that occur when multiple threads access shared data), ensures data integrity, and maintains consistent performance across all threads.<br><br>

          code: |
              <br>
              <span style="color: rgb(0, 255, 0);">def processBucket(<span style="color: rgb(172, 172, 255);">bucket: Dict, existing_db_buckets: Dict, existing_db_objects: Dict, account_id: str, current_keys: set, <span style="color: red;">objects_to_add: list[S3BUCKETOBJECTS]</span></span>)</span> -> None:
                  <span style="color: orange;">bucket_id</span> = <span style="color: rgb(172, 172, 255);">existing_db_buckets.get(bucket['bucket'])</span>

                  <span style="color: rgb(172, 172, 255);">thread_session</span> = <span style="color: rgb(0, 255, 0);">getThreadsafeDatabaseSession()</span>
                  if <span style="color: orange;">bucket_id</span>:
                      if <span style="color: rgb(0, 255, 0);">bucketNeedsUpdate(<span style="color: rgb(172, 172, 255);">thread_session, <span style="color: orange;">bucket_id</span>, bucket, account_id</span>)</span>:
                          <span style="color: rgb(0, 255, 0);">modifyBucket(<span style="color: rgb(172, 172, 255);">thread_session, <span style="color: orange;">bucket_id</span>, bucket, account_id</span>)</span>
                          logger.info(f'Bucket {bucket["bucket"]} updated.')
                  else:
                      logger.info(f'Adding bucket {bucket["bucket"]} to the database.')
                      bucket_id = addBucket(thread_session, bucket, account_id)
              <br>
        - content: |
              The function then iterates over all objects in the bucket, identifying each by a composite key (bucket ID and object key). Existing objects are compared using their <span style='color: blue;'>hash values</span> for efficient comparisons. Modified objects are updated immediately, while new objects are queued for batch addition later.
          code: |
              <br>
                  for <span style="color: rgb(172, 172, 255);">obj</span> in <span style="color: rgb(172, 172, 255);">bucket['objects']</span>:
                      <span style="color: orange;">object_key</span> = (<span style="color: rgb(172, 172, 255);">bucket_id, obj['key']</span>)
                      if <span style="color: orange;">object_key</span> in <span style="color: rgb(172, 172, 255);">existing_db_objects</span>:
                          <span style="color: rgb(172, 172, 255);">existing_object = existing_db_objects[object_key]</span>
                          if <span style="color: rgb(0, 255, 0);">objectNeedsUpdate(<span style="color: rgb(172, 172, 255);">existing_object, obj</span>)</span>:
                              <span style="color: rgb(0, 255, 0);">modifyObject(<span style="color: rgb(172, 172, 255);">thread_session, existing_object['id'], obj</span>)</span>
                      else:
                          <span style="color: rgb(0, 255, 0);"><span style="color: red;">objects_to_add</span>.append({
                              'bucket_id': <span style="color: rgb(172, 172, 255);">bucket_id</span>,
                              'obj': <span style="color: rgb(172, 172, 255);">obj</span>
                          })</span>
              <br>
        - content: |
              Lastly, a set of current object keys that were previously retrieved from AWS is incremented using a <span style='color: blue;'>set union</span> operation. This will be used with the object deletions similar to the object additions. After processing, the session commits changes and is removed.<br><br> 
          code: |
              <br>
                      <span style="color: rgb(172, 172, 255);">current_keys |= ({<span style="color: orange;">object_key</span>})</span>

                  logger.info(f'Existing object hashes in {bucket["bucket"]} checked.')

                  thread_session.commit()
                  thread_session.remove()
              <br><br>

    - title: Summary
      tabTitle: Summary
      subsections:
        - content: |
              <span style="color: rgb(0, 255, 0);">greencode</span>
              <span style="color: rgb(172, 172, 255);">blueishcode</span>
                <span style="color: red;">redcode</span>
                <span style="color: orange;">orangecode</span>
                <span style='color: blue;'>bluetext</span>
