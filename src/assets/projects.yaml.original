- id: backfill
  title: AWS Serverless Event-Driven Architecture
  subtitle: Event-driven architecture in AWS with EventBridge, Lambda, SQS, and S3.
  github: https://github.com/syuhas/awsdash-lambda
  description: |
    Developing an event-driven architecture in AWS with serverless microservices
  listIcon: /assets/project-images/event-images/event_icon3.png
  titleIcons:
    - /assets/project-images/event-images/event_icon.png
    - /assets/project-images/event-images/event_icon2.png
    - /assets/project-images/event-images/event_icon3.png
    - /assets/project-images/event-images/event_icon4.png
  sections:
    - title: Overview
      tabTitle: Overview
      subsections:
        - content: |

          imgs:
            - /assets/project-images/event-images/event.png

        - content: |
            This project demonstrates the implementation of a serverless, event-driven architecture in AWS, leveraging services such as EventBridge, Lambda, SQS, and S3 to process and respond to bucket and object-level events. By automating workflows triggered by predefined rules, it showcases the scalability and resilience of serverless solutions.<br><br>

            The events are divided into two main categories:

          listItems:
            - "<span style='color: blue;'>Bucket-Level Events</span>: Triggered by the creation or deletion of S3 buckets."
            - "<span style='color: blue;'>Object-Level Events</span>: Triggered by actions such as adding, modifying, or deleting objects within a bucket, including handling delete markers and post events."
          imgs:
        - content: |
            The separation of bucket-level and object-level events is necessary because S3 Event Notifications can only handle object-level events after a bucket is created and event notifications are explicitly enabled. I wanted to design this separation in so I could use S3 Event Notifications and treat the bucket-level events as an enrollment process.<br><br>
            For bucket-level events, CloudTrail captures logs for bucket creation or deletion, which are processed by EventBridge and sent downstream. For object-level events, S3 Event Notifications pass data events downstream for processing by additional AWS services.
            <br><br>
            In the following sections, I will provide a detailed overview of the architecture, including the setup of CloudTrail, EventBridge, SQS, and Lambda functions. I will also explain how the system processes bucket-level and object-level events, showcasing the end-to-end flow of the event-driven architecture.
            <br><br><br><br>
          imgs:
    - title: Bucket-Level Events Overview
      tabTitle: Buckets
      subsections:
        - content: |
            The bucket-level event pipeline is triggered by management events such as bucket creation or deletion, recorded in CloudTrail logs. These events are filtered using EventBridge, which routes relevant events, like bucket creation, to an SQS queue for further processing.<br><br>

            A Lambda function consumes events from the SQS queue, extracting metadata such as bucket name and associated account ID. This metadata is then stored in a PostgreSQL database, enabling detailed tracking and management of bucket-level activities. The Lambda function also enrolls newly created buckets for object-level event notifications, ensuring seamless integration with the next pipeline.
          code: |
            from typing import List, Dict, Union
            import boto3
            import json
            import boto3.session
            from loguru import logger
            from sqlalchemy import create_engine, select
            from sqlalchemy.orm import sessionmaker, declarative_base, relationship, scoped_session
            from sqlalchemy import Column, Integer, String, ForeignKey, DECIMAL, BigInteger
            from botocore.exceptions import ClientError
            from concurrent.futures import ThreadPoolExecutor
            import time

            account_lookup = [
                {
                    'sdlc': 'prod',
                    'account_id': '551796573889',
                    'region': 'us-east-1',
                    'role_arn': 'arn:aws:iam::551796573889:role/jenkinsAdminXacnt'
                },
                {
                    'sdlc': 'dev',
                    'account_id': '061039789243',
                    'region': 'us-east-1',
                    'role_arn': 'arn:aws:iam::061039789243:role/jenkinsAdminXacnt'
                }
            ]

            Base = declarative_base()
            class S3BUCKETS(Base):
                __tablename__ = 's3'
                id = Column(Integer, primary_key=True)
                account_id = Column(String)
                bucket = Column(String)
                totalSizeBytes = Column(BigInteger)
                totalSizeKb = Column(DECIMAL)
                totalSizeMb = Column(DECIMAL)
                totalSizeGb = Column(DECIMAL)
                costPerMonth = Column(DECIMAL)
                objects = relationship('S3BUCKETOBJECTS', backref='s3objects')
                def __repr__(self):
                    return f'Bucket {self.bucket}'

            class S3BUCKETOBJECTS(Base):
                __tablename__ = 's3objects'
                id = Column(Integer, primary_key=True)
                bucket_id = Column(Integer, ForeignKey('s3.id'))
                bucket = Column(String)
                key = Column(String)
                sizeBytes = Column(BigInteger)
                sizeKb = Column(DECIMAL)
                sizeMb = Column(DECIMAL)
                sizeGb = Column(DECIMAL)
                costPerMonth = Column(DECIMAL)

            def lambda_handler(event, context):
                logger.info("Starting backfill process...")
                start_time = time.time()

                for account in account_lookup:


                    logger.info("Retrieving existing buckets for {}...", account['account_id'])

                    session = getAccountSession(account)

                    buckets = getBucketsData(session)

                    logger.info("Backfilling database for {}...", account['account_id'])

                    backfillDatabase(buckets, account['account_id'])
                
                end_time = time.time()

                logger.info(f"Execution time: {end_time - start_time} seconds")


            def backfillDatabase(buckets: List[dict], account_id: str) -> dict:
                try:
                    current_keys = set()
                    session = getDatabaseSession()
                    existing_db_buckets = {b.bucket: b.id for b in session.query(S3BUCKETS).filter(S3BUCKETS.account_id == account_id).all()}
                    existing_db_objects = {(obj.bucket_id, obj.key): {
                                                "id": obj.id,
                                                "hash": hash((obj.sizeBytes, obj.costPerMonth))
                    } for obj in session.query(S3BUCKETOBJECTS).join(S3BUCKETS, S3BUCKETS.id == S3BUCKETOBJECTS.bucket_id).filter(S3BUCKETS.account_id == account_id).all()}

                    try:
                        with ThreadPoolExecutor() as executor:
                            futures = [
                                executor.submit(processBucket, bucket, existing_db_buckets, existing_db_objects, account_id, current_keys) for bucket in buckets
                            ]
                            for future in futures:
                                future.result()

                        current_buckets = {bucket['bucket'] for bucket in buckets}
                        
                    except Exception as e:
                        logger.exception(e)
                        session.rollback()
                        return {
                            'statusCode': 500,
                            'body': json.dumps(f'An error occurred: {e}')
                        }
                        

                    objects_to_delete = set(existing_db_objects.keys()) - current_keys
                    buckets_to_delete = set(existing_db_buckets.keys()) - current_buckets
                    for obj in objects_to_delete:
                        logger.info(f'Deleting object from database {obj}')
                        deleteObject(session, existing_db_objects[obj]['id'])
                    for bucket in buckets_to_delete:
                        logger.info(f'Deleting bucket from database {bucket}')
                        deleteBucket(session, existing_db_buckets[bucket])

                except Exception as e:
                    session.rollback()
                    return {
                        'statusCode': 500,
                        'body': json.dumps(f'An error occurred: {e}')
                    }
                finally:
                    session.commit()
                    session.close()



            def processBucket(bucket: Dict, existing_db_buckets: Dict, existing_db_objects: Dict, account_id: str, current_keys: set) -> None:
                bucket_id = existing_db_buckets.get(bucket['bucket'])
                try:
                    thread_session = getThreadsafeDatabaseSession()
                    if bucket_id:
                        logger.info(f'Bucket {bucket["bucket"]} already exists in the database.')
                        if bucketNeedsUpdate(thread_session, bucket_id, bucket, account_id):
                            modifyBucket(thread_session, bucket_id, bucket, account_id)
                            logger.info(f'Bucket {bucket["bucket"]} updated.')
                    else:
                        logger.info(f'Adding bucket {bucket["bucket"]} to the database.')
                        bucket_id = addBucket(thread_session, bucket, account_id)

                    for obj in bucket['objects']:
                        object_key = (bucket_id, obj['key'])
                        if object_key in existing_db_objects:
                            # logger.info(f'{obj["key"]} exists in the database.')
                            existing_object = existing_db_objects[object_key]
                            if objectNeedsUpdate(existing_object, obj):
                                modifyObject(thread_session, existing_object['id'], obj)

                        else:
                            logger.info(f'Adding object {obj["key"]} to the database.')
                            addObject(thread_session, bucket_id, obj)

                        current_keys |= ({(bucket_id, obj['key'])})
                    thread_session.commit()
                    thread_session.remove()

                except Exception as e:
                    logger.exception(e)




            #################################### Helper Functions for Buckets ##################################################

            def bucketNeedsUpdate(session, bucket_id: int, bucket: Dict, account_id: str) -> bool:
                existing_bucket = session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).one()
                return (
                    existing_bucket.account_id != account_id or
                    existing_bucket.totalSizeBytes != bucket['totalSizeBytes'] or
                    existing_bucket.totalSizeKb != bucket['totalSizeKb'] or
                    existing_bucket.totalSizeMb != bucket['totalSizeMb'] or
                    existing_bucket.totalSizeGb != bucket['totalSizeGb'] or
                    existing_bucket.costPerMonth != bucket['costPerMonth']
                )

            def modifyBucket(session, bucket_id: int, bucket: Dict, account_id: str) -> None:
                session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).update({
                    "account_id": account_id,
                    "totalSizeBytes": bucket["totalSizeBytes"],
                    "totalSizeKb": bucket["totalSizeKb"],
                    "totalSizeMb": bucket["totalSizeMb"],
                    "totalSizeGb": bucket["totalSizeGb"],
                    "costPerMonth": bucket["costPerMonth"],
                })
                session.commit()


            def addBucket(session: sessionmaker, bucket: Dict, account_id: str) -> str:
                new_bucket = S3BUCKETS(
                    account_id=account_id,
                    bucket=bucket['bucket'],
                    totalSizeBytes=bucket['totalSizeBytes'],
                    totalSizeKb=bucket['totalSizeKb'],
                    totalSizeMb=bucket['totalSizeMb'],
                    totalSizeGb=bucket['totalSizeGb'],
                    costPerMonth=bucket['costPerMonth']
                )
                session.add(new_bucket)
                session.commit()
                return new_bucket.id

            def deleteBucket(session: sessionmaker, bucket_id: int) -> None:
                session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.bucket_id == bucket_id).delete()
                session.query(S3BUCKETS).filter(S3BUCKETS.id == bucket_id).delete()
                session.commit()

            ##################################### Helper Functions for Objects #################################################

            def objectNeedsUpdate(existing_object, object: Dict) -> bool:
                return (
                    existing_object['hash'] != object['hash']
                )

            def modifyObject(session: sessionmaker, object_id: int, object: Dict) -> None:
                session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.id == object_id).update({
                    "sizeBytes": object["sizeBytes"],
                    "sizeKb": object["sizeKb"],
                    "sizeMb": object["sizeMb"],
                    "sizeGb": object["sizeGb"],
                    "costPerMonth": object["costPerMonth"]
                })
                session.commit()

            def addObject(session: sessionmaker, bucket_id: int, obj: Dict) -> None:
                new_object = S3BUCKETOBJECTS(
                    bucket_id=bucket_id,
                    bucket=obj['bucket'],
                    key=obj['key'],
                    sizeBytes=obj['sizeBytes'],
                    sizeKb=obj['sizeKb'],
                    sizeMb=obj['sizeMb'],
                    sizeGb=obj['sizeGb'],
                    costPerMonth=obj['costPerMonth']
                )
                session.add(new_object)
                session.commit()

            def deleteObject(session: sessionmaker, object_id: int) -> None:
                session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.id == object_id).delete()
                session.commit()

            ##################################### AWS Helper Functions #################################################

            def getAccountSession(account: dict) -> boto3.session.Session:
                session = boto3.Session()
                sts = session.client('sts')
                response = sts.assume_role(
                    RoleArn=account['role_arn'],
                    RoleSessionName='s3-backfill',
                    DurationSeconds=900
                )
                credentials = response['Credentials']
                account_session = boto3.Session(
                    aws_access_key_id=credentials['AccessKeyId'],
                    aws_secret_access_key=credentials['SecretAccessKey'],
                    aws_session_token=credentials['SessionToken'],
                    region_name=account['region']
                )
                return account_session

            def getBucketsData(session: boto3.session.Session) -> list:
                logger.info('Retrieving S3 bucket data from AWS...')
                bucket_list = []

                s3 = session.client('s3')
                buckets = s3.list_buckets()

                for bucket in buckets['Buckets']:
                    # if bucket['Name'].startswith('aws-cloudtrail'):
                    bucket_dict = {
                        'bucket': bucket['Name'],
                        'totalSizeBytes': 0,
                        'totalSizeKb': 0,
                        'totalSizeMb': 0,
                        'totalSizeGb': 0,
                        'costPerMonth': 0,
                        'objects': []
                    }
                    
                    paginator = s3.get_paginator('list_objects_v2')
                
                    object_list = []
                
                    total_bucket_cost = 0
                
                    object_iterator = paginator.paginate(Bucket=bucket['Name'])
                    for page in object_iterator:
                        if 'Contents' in page:
                            for obj in page['Contents']:
                                object_dict = {
                                    'key': obj['Key'],
                                    'bucket': bucket['Name'],
                                    'sizeBytes': obj['Size'],
                                    'sizeKb': round(obj['Size'] / 1024, 2),  # Converts bytes to KB
                                    'sizeMb': round(obj['Size'] / (1024 * 1024), 2),  # Converts bytes to MB
                                    'sizeGb': round(obj['Size'] / (1024 * 1024 * 1024), 4),  # Converts bytes to GB
                                }
                                object_dict['costPerMonth'] = object_dict['sizeGb'] * 0.023
                                object_dict['hash'] = hash((object_dict['sizeBytes'], object_dict['costPerMonth']))
                                total_bucket_cost = total_bucket_cost + object_dict['costPerMonth']
                                object_list.append(object_dict)

                    bucket_dict['totalSizeBytes'] = sum([obj['sizeBytes'] for obj in object_list])
                    bucket_dict['totalSizeKb'] = round(sum([obj['sizeKb'] for obj in object_list]), 4)
                    bucket_dict['totalSizeMb'] = round(sum([obj['sizeMb'] for obj in object_list]), 4)
                    bucket_dict['totalSizeGb'] = round(sum([obj['sizeGb'] for obj in object_list]), 4)
                    bucket_dict['costPerMonth'] = round(total_bucket_cost, 6)
                    bucket_dict['objects'] = object_list
                    bucket_list.append(bucket_dict)

                logger.info('S3 bucket data retrieved.')
                return bucket_list

            def getDatabaseCredentials() -> dict:
                secret_id = "arn:aws:secretsmanager:us-east-1:061039789243:secret:rds!db-555390f8-60f2-4d37-ad75-e63d8f0cbfa9-0s9oyX"
                region = "us-east-1"
                session = boto3.session.Session()
                client = session.client('secretsmanager', region_name=region)

                try:
                    secret_response = client.get_secret_value(SecretId=secret_id)
                    secret = secret_response['SecretString']
                    json_secret = json.loads(secret)
                    credentials = {
                        'username': json_secret['username'],
                        'password': json_secret['password']
                    }
                    return credentials
                except ClientError as e:
                    raise e

            ##################################### Database Helper Functions #################################################

            def getEngine() -> create_engine:
                credentials = getDatabaseCredentials()
                engine = create_engine(
                    f'postgresql://{credentials["username"]}:{credentials["password"]}@resources.czmo2wqo0w7e.us-east-1.rds.amazonaws.com:5432'
                )
                return engine

            def getDatabaseSession() -> sessionmaker:
                engine = getEngine()
                Session = sessionmaker(bind=engine)
                session = Session()
                # session_factory = sessionmaker(bind=engine)
                # session = scoped_session(session_factory)
                return session

            def getThreadsafeDatabaseSession() -> scoped_session:
                engine = getEngine()
                # Session = sessionmaker(bind=engine)
                # session = Session()
                session_factory = sessionmaker(bind=engine)
                session = scoped_session(session_factory)
                return session

            def getBuckets(account_id: str) -> list:
                session = getDatabaseSession()
                result = session.query(S3BUCKETS).filter(S3BUCKETS.account_id == account_id).all()
                session.close()
                return result

            def getObjectsForBucket(bucket_id: int) -> list:
                session = getDatabaseSession()
                result = session.query(S3BUCKETOBJECTS).filter(S3BUCKETOBJECTS.bucket_id == bucket_id).all()
                session.close()
                return result
          imgs:
            - /assets/project-images/event-images/event_bucket.png

    - title: Configuring CloudTrail Logging
      tabTitle: CloudTrail
      subsections:
        - content: |
            Setting up a CloudTrail trail is a crucial first step in this event-driven architecture. The primary purpose of this trail is to capture and log management events for S3 buckets, enabling integration with EventBridge. This integration allows EventBridge to read these events and trigger workflows based on bucket-level activities such as creating or deleting a bucket.<br><br>
            In this setup, the CloudTrail trail is configured to log management events, with particular emphasis on Write API operations. These logs are delivered to a designated S3 bucket, where they can be accessed and processed. By enabling this trail, I ensure that EventBridge has the necessary visibility into bucket-level actions, which forms the foundation of the bucket pipeline.<br><br>
          imgs:
            - /assets/project-images/event-images/event1_1.png
            - /assets/project-images/event-images/event1_2.png
        - content: |
            Below is the bucket that the CloudTrail logs are delivered to. This trail is enabled organization-wide, so all S3 management events from all accounts in the org are captured. These logs will be used by EventBridge and filtered by CreateBucket and DeleteBucket events.
          imgs:
            - /assets/project-images/event-images/event1_3.png

    - title: Configuring EventBridge Rules
      tabTitle: EventBridge
      subsections:
        - content: |
            In this setup, I created an EventBridge rule to filter out S3 bucket management events specifically for CreateBucket and DeleteBucket actions. The event pattern is configured to match these actions in the CloudTrail logs, ensuring that only relevant events trigger the rule. This allows for precise control over which bucket-level activities are passed through the pipeline.<br><br>
          imgs:
            - /assets/project-images/event-images/event2_1.png
        - content: |
            Once the filtering rule is in place, the events are routed to an SQS queue. The SQS queue serves as a reliable target for EventBridge, providing a decoupled mechanism for handling the events. This ensures that the events can be processed asynchronously, even if the downstream services experience temporary unavailability.<br><br>

            The combination of EventBridge rules and the SQS target creates a streamlined event-driven workflow for bucket-level events, efficiently filtering, routing, and queuing S3 management events for further processing.
          imgs:
            - /assets/project-images/event-images/event2_2.png

    - title: SQS Event Queue for Bucket Events
      tabTitle: SQS - Buckets
      subsections:
        - content: |
            With the EventBridge rules in place, the next step involves setting up the SQS queue to handle bucket-level events. SQS serves as the intermediary messaging layer, ensuring reliable delivery of events and allowing for asychhronous decoupled processing by downstream services.<br><br>
          imgs:
            - /assets/project-images/event-images/event3_2.png

        - content: |
            The first step in this configuration is to create an SQS queue and define the necessary settings, such as message retention period and visibility timeout, to suit the needs of the pipeline. The SQS details page showcases these settings and ensures that the queue is configured to handle the expected traffic effectively.<br><br>
          imgs:
            - /assets/project-images/event-images/event3_1.png
            - /assets/project-images/event-images/event3_3.png

        - content: |
            An example payload of a CreateBucket event demonstrates the data structure being passed to the SQS queue. A Lambda target is configured to ingest and process the message. This payload includes critical details such as the event name, bucket name, timestamp, and other metadata, ensuring that the Lambda function receives all the information needed to process the event effectively.<br><br>
          imgs:
            - /assets/project-images/event-images/event3_4.png

    - title: Lambda Bucket Manager Function
      tabTitle: Lambda - Buckets
      subsections:
        - content: |
            The Lambda Bucket Manager Function processes bucket-level events received from the SQS queue. For each event, it extracts critical metadata, such as the bucket name and event type, to track and monitor storage activity.<br><br>
          imgs:
            - /assets/project-images/event-images/event4_1.png

        - content: |
            The main purpose of the function is to add new buckets to the database and enable S3 Event Notifications to enroll the bucket in the object-level event pipeline. The object-level function will compute the storage metrics for the buckets and objects whereas this function acts as more of an enrollment process.<br><br>

            The function supports event types like CreateBucket and DeleteBucket. For creation, it registers the bucket in the database and configures event notifications. For deletion, it removes the bucket and associated object data from the database.<br><br>

            These tasks ensure efficient bucket management and seamless integration with the object-level event pipeline.
          imgs:
            - /assets/project-images/event-images/event4_2.png
            - /assets/project-images/event-images/event4_3.png

    - title: Object-Level Events Overview
      tabTitle: Objects
      subsections:
        - content: |
            The object-level event pipeline handles events such as object creation, deletion, or modification within an S3 bucket. This process is triggered by S3 Event Notifications, which are configured on enrolled buckets. These notifications send event details, such as object metadata, to an SQS queue.<br><br>

            A Lambda function processes these events by extracting critical information, including object size (in bytes, kilobytes, megabytes, etc.) and calculating the storage cost. The extracted data is then stored in a PostgreSQL database for further analysis and management. This pipeline ensures efficient and automated handling of object-level activities, providing real-time updates and insights.
          imgs:
            - /assets/project-images/event-images/event_object.png

    - title: S3 Event Notifications
      tabTitle: S3 Events
      subsections:
        - content: |
            As part of the bucket-level pipeline, S3 Event Notifications are enabled for each new bucket to capture specific events, such as object Put, Post, and Delete, including delete markers. These notifications are configured via a Lambda function to ensure only relevant events are processed.<br><br>
          imgs:
            - /assets/project-images/event-images/event5_1.png
        - content: |
            The Event Notifications publish events to an SQS queue, enabling asynchronous processing by downstream Lambda functions. This logical separation ensures efficient handling of object-level events, independent of the bucket-level events.
          imgs:
            - /assets/project-images/event-images/event5_1.png
    - title: SQS Event Queue for Object Events
      tabTitle: SQS - Objects
      subsections:
        - content: |
            Again, SQS is used here to handle asynchronous event processing from S3 notifications, ensuring scalability and fault tolerance for downstream Lambda functions.<br><br>
            The SQS queue is configured with key settings such as visibility timeout, message retention, encryption, and access permissions. These configurations ensure secure, reliable, and efficient handling of event messages generated by S3.
          imgs:
            - /assets/project-images/event-images/event6_2.png
        - content: |
            Once configured, the SQS queue is integrated with a Lambda function target. This setup enables the Lambda function to process messages from the queue, triggering event-driven workflows to handle object-level events efficiently.
          imgs:
            - /assets/project-images/event-images/event6_1.png
            - /assets/project-images/event-images/event6_3.png
        - content: |
            To enhance reliability, a dead-letter queue (DLQ) is configured to capture any messages that fail to process, which I can capture and process separately or log for analytics. Additionally, an example payload demonstrates the structure of the messages sent to the SQS queue, highlighting the details provided for each event.
          imgs:
            - /assets/project-images/event-images/event6_4.png
        - content: |
          imgs:
            - /assets/project-images/event-images/event6_5.png
        - content: |
            The asynchronous nature of SQS decouples the event source (S3) from the processing logic (Lambda), allowing the system to handle high-throughput events without being constrained by downstream processing capacity.<br><br>
          imgs:

    - title: Lambda Object Manager Function
      tabTitle: Lambda - Objects
      subsections:
        - content: |
            This section highlights the object-level Lambda function, which is triggered by SQS messages from the event queue. This is the main data handler for this architecture. Since most operations will be object-level events, this is doing most of the heavy lifting. The function is configured with an SQS trigger, as shown in the image, and both function in this architecture are deployed using Jenkins and Terraform, with the deployment details covered in the next section.
          imgs:
            - /assets/project-images/event-images/event7_1.png

        - content: |
            The provided code demonstrates how the function processes the events described earlier in this project. The function parses the SQS message to extract the event details, including the event name, bucket name, and object key. Based on the event type, it performs the following actions:
          listItems:
            - "<span style='color: blue;'>Add/Remove/Modify Object</span>"
            - "<span style='color: blue;'>Calculate and Add/Update Metrics for the Object</span>"
            - "<span style='color: blue;'>Calculate and Update Metrics for the Bucket</span>"
          imgs:
            - /assets/project-images/event-images/event7_2.png
        - content: |
            The function calculates the size of the object in kilobytes (KB), megabytes (MB), and gigabytes (GB).<br><br>
            It computes the total cost for the object and updates the corresponding metrics in the database.<br><br>
            Additionally, it updates the cumulative totals for these metrics for the bucket.<br><br>
            This Lambda function serves as the core processing component of the event-driven architecture, ensuring accurate database updates based on bucket and object-level events.
          imgs:

    - title: Deploying Lambdas with Jenkins and Terraform
      tabTitle: Deploy
      subsections:
        - content: |
            To deploy the Lambda functions efficiently, I am using a combination of Jenkins and Terraform. Jenkins handles the automation of the deployment pipeline, while Terraform provisions the required infrastructure, ensuring a streamlined and repeatable process.
            <br><br>
            The following Jenkins pipelines show the overview of the deployment process, orchestrating the packaging, provisioning and deployment steps.
          imgs:
            - /assets/project-images/event-images/event8_1.png
            - /assets/project-images/event-images/event8_2.png
        - content: |
            Terraform is used to manage the Lambda infrastructure and employs code hashing to detect changes, ensuring only updated functions are deployed. A bash script packages the Lambda functions, preparing them for deployment.
          imgs:

        - content: |

          imgs:
            - /assets/project-images/event-images/event8_3.png
            - /assets/project-images/event-images/event8_4.png
        - content: |
            The Jenkinsfile defines and executes the pipeline's stages, coordinating infrastructure provisioning and Lambda deployment for a streamlined workflow.
          imgs:
            - /assets/project-images/event-images/event8_5.png

    - title: Summary
      tabTitle: Summary
      subsections:
        - content: |
            This project was a valuable learning experience that brought together multiple AWS services into a cohesive event-driven architecture. It began with the challenge of automating the enrollment of S3 buckets and evolved into a full-scale pipeline capable of processing both bucket-level and object-level events efficiently. From setting up CloudTrail and EventBridge to leveraging SQS and Lambda functions, every step of the pipeline presented opportunities to deepen my understanding of event-driven design and AWS integration.

            <br><br>

            One of the most rewarding aspects of the project was working with Lambda functions to process and enrich event data. This included calculating storage metrics and costs for S3 objects and enrolling buckets for additional event notifications. Integrating PostgreSQL to store the processed data added a layer of persistence and visibility to the workflow, allowing for more meaningful insights into bucket usage and object-level changes.

            <br><br>

            Deploying the entire architecture with Jenkins and Terraform was another critical milestone. Building a CI/CD pipeline not only streamlined the deployment process but also ensured infrastructure consistency and reliability. Incorporating a bash script to package Lambda functions and leveraging Terraformâ€™s state management and hashing features to track changes further emphasized the importance of automation and reproducibility.

            <br><br>

            Reflecting on the project, I gained significant experience in designing and deploying event-driven architectures. I also learned the importance of careful planning when integrating multiple AWS services to ensure seamless communication and error handling. This project demonstrated the potential for scalability and highlighted areas for future exploration, such as optimizing notification workflows or adding advanced monitoring capabilities.

            <br><br>

            Overall, this project was not only a technical accomplishment but also a major step forward in understanding how to build efficient, scalable, and cost-effective serverless solutions. It has laid the groundwork for future enhancements and provided a solid foundation for tackling more complex event-driven systems.
          imgs:

- id: event
  title: AWS Serverless Event-Driven Architecture
  subtitle: Event-driven architecture in AWS with EventBridge, Lambda, SQS, and S3.
  github: https://github.com/syuhas/awsdash-lambda
  description: |
    Developing an event-driven architecture in AWS with serverless microservices
  listIcon: /assets/project-images/event-images/event_icon3.png
  titleIcons:
    - /assets/project-images/event-images/event_icon.png
    - /assets/project-images/event-images/event_icon2.png
    - /assets/project-images/event-images/event_icon3.png
    - /assets/project-images/event-images/event_icon4.png
  sections:
    - title: Overview
      tabTitle: Overview
      subsections:
        - content: |

          imgs:
            - /assets/project-images/event-images/event.png

        - content: |
            This project demonstrates the implementation of a serverless, event-driven architecture in AWS, leveraging services such as EventBridge, Lambda, SQS, and S3 to process and respond to bucket and object-level events. By automating workflows triggered by predefined rules, it showcases the scalability and resilience of serverless solutions.<br><br>

            The events are divided into two main categories:

          listItems:
            - "<span style='color: blue;'>Bucket-Level Events</span>: Triggered by the creation or deletion of S3 buckets."
            - "<span style='color: blue;'>Object-Level Events</span>: Triggered by actions such as adding, modifying, or deleting objects within a bucket, including handling delete markers and post events."
          imgs:
        - content: |
            The separation of bucket-level and object-level events is necessary because S3 Event Notifications can only handle object-level events after a bucket is created and event notifications are explicitly enabled. I wanted to design this separation in so I could use S3 Event Notifications and treat the bucket-level events as an enrollment process.<br><br>
            For bucket-level events, CloudTrail captures logs for bucket creation or deletion, which are processed by EventBridge and sent downstream. For object-level events, S3 Event Notifications pass data events downstream for processing by additional AWS services.
            <br><br>
            In the following sections, I will provide a detailed overview of the architecture, including the setup of CloudTrail, EventBridge, SQS, and Lambda functions. I will also explain how the system processes bucket-level and object-level events, showcasing the end-to-end flow of the event-driven architecture.
            <br><br><br><br>
          imgs:
    - title: Bucket-Level Events Overview
      tabTitle: Buckets
      subsections:
        - content: |
            The bucket-level event pipeline is triggered by management events such as bucket creation or deletion, recorded in CloudTrail logs. These events are filtered using EventBridge, which routes relevant events, like bucket creation, to an SQS queue for further processing.<br><br>

            A Lambda function consumes events from the SQS queue, extracting metadata such as bucket name and associated account ID. This metadata is then stored in a PostgreSQL database, enabling detailed tracking and management of bucket-level activities. The Lambda function also enrolls newly created buckets for object-level event notifications, ensuring seamless integration with the next pipeline.
          imgs:
            - /assets/project-images/event-images/event_bucket.png

    - title: Configuring CloudTrail Logging
      tabTitle: CloudTrail
      subsections:
        - content: |
            Setting up a CloudTrail trail is a crucial first step in this event-driven architecture. The primary purpose of this trail is to capture and log management events for S3 buckets, enabling integration with EventBridge. This integration allows EventBridge to read these events and trigger workflows based on bucket-level activities such as creating or deleting a bucket.<br><br>
            In this setup, the CloudTrail trail is configured to log management events, with particular emphasis on Write API operations. These logs are delivered to a designated S3 bucket, where they can be accessed and processed. By enabling this trail, I ensure that EventBridge has the necessary visibility into bucket-level actions, which forms the foundation of the bucket pipeline.<br><br>
          imgs:
            - /assets/project-images/event-images/event1_1.png
            - /assets/project-images/event-images/event1_2.png
        - content: |
            Below is the bucket that the CloudTrail logs are delivered to. This trail is enabled organization-wide, so all S3 management events from all accounts in the org are captured. These logs will be used by EventBridge and filtered by CreateBucket and DeleteBucket events.
          imgs:
            - /assets/project-images/event-images/event1_3.png

    - title: Configuring EventBridge Rules
      tabTitle: EventBridge
      subsections:
        - content: |
            In this setup, I created an EventBridge rule to filter out S3 bucket management events specifically for CreateBucket and DeleteBucket actions. The event pattern is configured to match these actions in the CloudTrail logs, ensuring that only relevant events trigger the rule. This allows for precise control over which bucket-level activities are passed through the pipeline.<br><br>
          imgs:
            - /assets/project-images/event-images/event2_1.png
        - content: |
            Once the filtering rule is in place, the events are routed to an SQS queue. The SQS queue serves as a reliable target for EventBridge, providing a decoupled mechanism for handling the events. This ensures that the events can be processed asynchronously, even if the downstream services experience temporary unavailability.<br><br>

            The combination of EventBridge rules and the SQS target creates a streamlined event-driven workflow for bucket-level events, efficiently filtering, routing, and queuing S3 management events for further processing.
          imgs:
            - /assets/project-images/event-images/event2_2.png

    - title: SQS Event Queue for Bucket Events
      tabTitle: SQS - Buckets
      subsections:
        - content: |
            With the EventBridge rules in place, the next step involves setting up the SQS queue to handle bucket-level events. SQS serves as the intermediary messaging layer, ensuring reliable delivery of events and allowing for asychhronous decoupled processing by downstream services.<br><br>
          imgs:
            - /assets/project-images/event-images/event3_2.png

        - content: |
            The first step in this configuration is to create an SQS queue and define the necessary settings, such as message retention period and visibility timeout, to suit the needs of the pipeline. The SQS details page showcases these settings and ensures that the queue is configured to handle the expected traffic effectively.<br><br>
          imgs:
            - /assets/project-images/event-images/event3_1.png
            - /assets/project-images/event-images/event3_3.png

        - content: |
            An example payload of a CreateBucket event demonstrates the data structure being passed to the SQS queue. A Lambda target is configured to ingest and process the message. This payload includes critical details such as the event name, bucket name, timestamp, and other metadata, ensuring that the Lambda function receives all the information needed to process the event effectively.<br><br>
          imgs:
            - /assets/project-images/event-images/event3_4.png

    - title: Lambda Bucket Manager Function
      tabTitle: Lambda - Buckets
      subsections:
        - content: |
            The Lambda Bucket Manager Function processes bucket-level events received from the SQS queue. For each event, it extracts critical metadata, such as the bucket name and event type, to track and monitor storage activity.<br><br>
          imgs:
            - /assets/project-images/event-images/event4_1.png

        - content: |
            The main purpose of the function is to add new buckets to the database and enable S3 Event Notifications to enroll the bucket in the object-level event pipeline. The object-level function will compute the storage metrics for the buckets and objects whereas this function acts as more of an enrollment process.<br><br>

            The function supports event types like CreateBucket and DeleteBucket. For creation, it registers the bucket in the database and configures event notifications. For deletion, it removes the bucket and associated object data from the database.<br><br>

            These tasks ensure efficient bucket management and seamless integration with the object-level event pipeline.
          imgs:
            - /assets/project-images/event-images/event4_2.png
            - /assets/project-images/event-images/event4_3.png

    - title: Object-Level Events Overview
      tabTitle: Objects
      subsections:
        - content: |
            The object-level event pipeline handles events such as object creation, deletion, or modification within an S3 bucket. This process is triggered by S3 Event Notifications, which are configured on enrolled buckets. These notifications send event details, such as object metadata, to an SQS queue.<br><br>

            A Lambda function processes these events by extracting critical information, including object size (in bytes, kilobytes, megabytes, etc.) and calculating the storage cost. The extracted data is then stored in a PostgreSQL database for further analysis and management. This pipeline ensures efficient and automated handling of object-level activities, providing real-time updates and insights.
          imgs:
            - /assets/project-images/event-images/event_object.png

    - title: S3 Event Notifications
      tabTitle: S3 Events
      subsections:
        - content: |
            As part of the bucket-level pipeline, S3 Event Notifications are enabled for each new bucket to capture specific events, such as object Put, Post, and Delete, including delete markers. These notifications are configured via a Lambda function to ensure only relevant events are processed.<br><br>
          imgs:
            - /assets/project-images/event-images/event5_1.png
        - content: |
            The Event Notifications publish events to an SQS queue, enabling asynchronous processing by downstream Lambda functions. This logical separation ensures efficient handling of object-level events, independent of the bucket-level events.
          imgs:
            - /assets/project-images/event-images/event5_1.png
    - title: SQS Event Queue for Object Events
      tabTitle: SQS - Objects
      subsections:
        - content: |
            Again, SQS is used here to handle asynchronous event processing from S3 notifications, ensuring scalability and fault tolerance for downstream Lambda functions.<br><br>
            The SQS queue is configured with key settings such as visibility timeout, message retention, encryption, and access permissions. These configurations ensure secure, reliable, and efficient handling of event messages generated by S3.
          imgs:
            - /assets/project-images/event-images/event6_2.png
        - content: |
            Once configured, the SQS queue is integrated with a Lambda function target. This setup enables the Lambda function to process messages from the queue, triggering event-driven workflows to handle object-level events efficiently.
          imgs:
            - /assets/project-images/event-images/event6_1.png
            - /assets/project-images/event-images/event6_3.png
        - content: |
            To enhance reliability, a dead-letter queue (DLQ) is configured to capture any messages that fail to process, which I can capture and process separately or log for analytics. Additionally, an example payload demonstrates the structure of the messages sent to the SQS queue, highlighting the details provided for each event.
          imgs:
            - /assets/project-images/event-images/event6_4.png
        - content: |
          imgs:
            - /assets/project-images/event-images/event6_5.png
        - content: |
            The asynchronous nature of SQS decouples the event source (S3) from the processing logic (Lambda), allowing the system to handle high-throughput events without being constrained by downstream processing capacity.<br><br>
          imgs:

    - title: Lambda Object Manager Function
      tabTitle: Lambda - Objects
      subsections:
        - content: |
            This section highlights the object-level Lambda function, which is triggered by SQS messages from the event queue. This is the main data handler for this architecture. Since most operations will be object-level events, this is doing most of the heavy lifting. The function is configured with an SQS trigger, as shown in the image, and both function in this architecture are deployed using Jenkins and Terraform, with the deployment details covered in the next section.
          imgs:
            - /assets/project-images/event-images/event7_1.png

        - content: |
            The provided code demonstrates how the function processes the events described earlier in this project. The function parses the SQS message to extract the event details, including the event name, bucket name, and object key. Based on the event type, it performs the following actions:
          listItems:
            - "<span style='color: blue;'>Add/Remove/Modify Object</span>"
            - "<span style='color: blue;'>Calculate and Add/Update Metrics for the Object</span>"
            - "<span style='color: blue;'>Calculate and Update Metrics for the Bucket</span>"
          imgs:
            - /assets/project-images/event-images/event7_2.png
        - content: |
            The function calculates the size of the object in kilobytes (KB), megabytes (MB), and gigabytes (GB).<br><br>
            It computes the total cost for the object and updates the corresponding metrics in the database.<br><br>
            Additionally, it updates the cumulative totals for these metrics for the bucket.<br><br>
            This Lambda function serves as the core processing component of the event-driven architecture, ensuring accurate database updates based on bucket and object-level events.
          imgs:

    - title: Deploying Lambdas with Jenkins and Terraform
      tabTitle: Deploy
      subsections:
        - content: |
            To deploy the Lambda functions efficiently, I am using a combination of Jenkins and Terraform. Jenkins handles the automation of the deployment pipeline, while Terraform provisions the required infrastructure, ensuring a streamlined and repeatable process.
            <br><br>
            The following Jenkins pipelines show the overview of the deployment process, orchestrating the packaging, provisioning and deployment steps.
          imgs:
            - /assets/project-images/event-images/event8_1.png
            - /assets/project-images/event-images/event8_2.png
        - content: |
            Terraform is used to manage the Lambda infrastructure and employs code hashing to detect changes, ensuring only updated functions are deployed. A bash script packages the Lambda functions, preparing them for deployment.
          imgs:

        - content: |

          imgs:
            - /assets/project-images/event-images/event8_3.png
            - /assets/project-images/event-images/event8_4.png
        - content: |
            The Jenkinsfile defines and executes the pipeline's stages, coordinating infrastructure provisioning and Lambda deployment for a streamlined workflow.
          imgs:
            - /assets/project-images/event-images/event8_5.png

    - title: Summary
      tabTitle: Summary
      subsections:
        - content: |
            This project was a valuable learning experience that brought together multiple AWS services into a cohesive event-driven architecture. It began with the challenge of automating the enrollment of S3 buckets and evolved into a full-scale pipeline capable of processing both bucket-level and object-level events efficiently. From setting up CloudTrail and EventBridge to leveraging SQS and Lambda functions, every step of the pipeline presented opportunities to deepen my understanding of event-driven design and AWS integration.

            <br><br>

            One of the most rewarding aspects of the project was working with Lambda functions to process and enrich event data. This included calculating storage metrics and costs for S3 objects and enrolling buckets for additional event notifications. Integrating PostgreSQL to store the processed data added a layer of persistence and visibility to the workflow, allowing for more meaningful insights into bucket usage and object-level changes.

            <br><br>

            Deploying the entire architecture with Jenkins and Terraform was another critical milestone. Building a CI/CD pipeline not only streamlined the deployment process but also ensured infrastructure consistency and reliability. Incorporating a bash script to package Lambda functions and leveraging Terraformâ€™s state management and hashing features to track changes further emphasized the importance of automation and reproducibility.

            <br><br>

            Reflecting on the project, I gained significant experience in designing and deploying event-driven architectures. I also learned the importance of careful planning when integrating multiple AWS services to ensure seamless communication and error handling. This project demonstrated the potential for scalability and highlighted areas for future exploration, such as optimizing notification workflows or adding advanced monitoring capabilities.

            <br><br>

            Overall, this project was not only a technical accomplishment but also a major step forward in understanding how to build efficient, scalable, and cost-effective serverless solutions. It has laid the groundwork for future enhancements and provided a solid foundation for tackling more complex event-driven systems.
          imgs:

- id: rproxy
  title: Reverse Proxy - Nginx with Docker, FastAPI, and Angular
  subtitle: Serving a FastAPI backend and Angular frontend with Nginx reverse proxy in Docker containers.
  github: https://github.com/syuhas/websitev2
  description: |
    Configuring a reverse proxy to serve a FastAPI backend and Angular frontend on the same EC2 instance and domain.
  listIcon: /assets/project-images/rproxy-images/rproxy_icon2.png
  titleIcons:
    - /assets/project-images/rproxy-images/rproxy_icon2.png
    - /assets/project-images/rproxy-images/rproxy_icon.png
    - /assets/project-images/rproxy-images/rproxy_icon3.png
  sections:
    - title: Overview
      tabTitle: Overview
      subsections:
        - content: |

          imgs:
            - /assets/project-images/rproxy-images/rproxy.png

        - content: |
            This project explores the creation and configuration of a reverse proxy to serve my backend FastAPI application and frontend Angular application in separate Docker containers on the same domain. The backend API is part of a larger project I was working on for an S3 dashboard and is built using FastAPI to pull resource metadata from a PostgreSQL database. The front end is this website, which represents the second version of my personal site, built with Angular.
            <br><br>
            The goal of this project was to understand how reverse proxies work and how I could use one to route all traffic with the /api path to my API. Additionally, I wanted to explore the cost-saving benefits of using a reverse proxy and self-managed SSL certificates, rather than hosting the API on a separate subdomain. A reverse proxy is a versatile design pattern that offers various advantages, including load balancing, enhanced security, SSL termination, and simplified migrations. This project also incorporates tools like Terraform for provisioning infrastructure in AWS in a repeatable and consistent way, LetsEncrypt for SSL certificates, and Jenkins combined with Bash scripting for automation and CI/CD.
          imgs:

    - title: Defining Infrastructure with Terraform
      tabTitle: Terraform
      subsections:
        - content: |
            I defined this project's infrastructure in AWS using Terraform. Terraform is a tool that will allow me to define and provision infrastructure as code in a repeatable and consistent way. In addition, using a backend state configuration stored in S3 and DynamoDB, I can keep track of the state of my infrastructure and Terraform will automatically detect changes and allow me to update the infrastructure as needed, or update the application without having to reprovision the entire infrastructure.<br><br>

            Terraform can access AWS and build resources by accessing the IMDSv2 (Instance Metadata Service) attached directly to the Jenkins build server that is dynamically provisioned and shut down as needed (check out <a href="/projects/jenkins?tab=Overview">Provisioning Build Servers in Jenkins</a>). This approach eliminates the need to pass sensitive credentials to Jenkins or store them in the code.<br><br>


            The main components of the infrastructure include:
          listItems:
            - An EC2 instance to run the website and API with the reverse proxy
            - A security group to allow the correct ports for inbound traffic
            - DNS and records to route traffic to the EC2 instance
          imgs:
            - /assets/project-images/rproxy-images/prox1_1.png

        - content: |
            In my Jenkinsfile, I initialize the backend configuration, which allows for the use of variables. During this process, I can define TF_VAR variables that are utilized in the Terraform configuration file. This approach is particularly useful for handling sensitive information that I prefer not to hard code into the configuration, as well as for referencing dynamic resources such as subnets or security groups that I want to retrieve from Jenkins via a lookup. However, since this project has a relatively simple setup, I have opted to define the variables directly within the Jenkinsfile for convenience.
          imgs:
            - /assets/project-images/rproxy-images/prox1_2.png

    - title: Creating SSL Certificates with LetsEncrypt
      tabTitle: SSL/TLS
      subsections:
        - content: |
            To avoid relying on AWS Load Balancing and to reduce costs, I opted to create my own SSL certificates using third-party tools. Let's Encrypt is a free service that enables the creation of domain-validated SSL certificates. It employs a CLI tool called Certbot to generate the certificates, which can then be integrated into your server configuration to validate the domain and encrypt traffic. These certificates are valid for 90 days and can be renewed automatically using a cron job. This approach not only helps save costs but also provides a valuable opportunity to learn how to create and manage SSL certificates.<br><br>

            Certbot also supports Route53 integration, which automates the DNS validation process. Since I use Route53 for my domain, this feature simplifies the validation and certificate creation steps. In the CLI command shown below, I instruct Certbot to utilize the Route53 plugin to generate and validate SSL certificates for my <strong style="color: blue;">digitalsteve.net</strong> and <strong style="color: blue;">www.digitalsteve.net</strong>. Certbot initiates a DNS TXT challenge to Route53, waits for the response to validate the domain, and then creates the certificates. These certificates are ready to be incorporated into any server configuration associated with the validated domains.
          imgs:
            - /assets/project-images/rproxy-images/prox2_1.gif

        - content: |
            Now that I have created my SSL certificates, I can integrate them into my Nginx configuration to encrypt traffic between the client and the server. For this project, I have included the certificates with the deployment files so that Jenkins can access them and copy them to the server. Specifically, the fullchain.pem and privkey.pem files are the certificate and private key, respectively, that I will use in the Nginx configuration to ensure secure traffic encryption.<br><br>

            In a future project, I plan to explore storing the certificates in AWS Secrets Manager or S3 and retrieving them directly from there during deployment. This approach would provide a more secure way to store the certificates and simplify their management and renewal process over time.
          imgs:
            - /assets/project-images/rproxy-images/prox2_2.png

    - title: Applications - FastAPI and Angular
      tabTitle: Apps
      subsections:
        - content: |
            The front end of this setup is the very website you are on, built using Angular 17. The backend of this reverse proxy configuration can be accessed through the /api path and hosts an API built with FastAPI. This API includes various endpoints that I plan to leverage in my future projects.<br><br>

            The API retrieves metadata about AWS S3 resources, which is stored in a <strong>Postgres</strong> database. It is developed using FastAPI and served via <strong>uvicorn</strong>. The reverse proxy configuration in Nginx enables seamless access to the API through the /api path of this website.<br><br>

            You can explore the <a href="/projects/rproxy?tab=Swagger">Swagger API Documentation</a> in the next section.
          imgs:
            - /assets/project-images/rproxy-images/prox3_3.png
            - /assets/project-images/rproxy-images/prox3_4.png
        - content: |
            My Angular application is developed using Angular 17 and features a straightforward, user-friendly design. The application serves as my base of operations, showcasing my portfolio, resume, and projects. To streamline content management, I utilize templating through YAML files, allowing for easy addition and display of new projects. I plan to delve deeper into this templating approach in a future project.
          imgs:
            - /assets/project-images/rproxy-images/prox3_1.png
            - /assets/project-images/rproxy-images/prox3_2.png

    - title: Test Out the Swagger API
      tabTitle: Swagger
      subsections:
        - content: |
            <iframe src="https://www.digitalsteve.net/api/docs" style="width: 80vw; height: 100vh; border: none;"></iframe>

          imgs:

    - title: Configuring Docker for Angular and Python
      tabTitle: Docker
      subsections:
        - content: |
            I am using <strong>Docker</strong> to containerize my applications for this project. Docker enables me to package all the code, dependencies, and configurations into a single image that can run on any machine with Docker installed. In this setup, one of the containers hosts both the Angular application and the reverse proxy. Given the smaller scale of this project, I opted for this design instead of deploying a dedicated Nginx container for the reverse proxy.<br><br>

          imgs:
            - /assets/project-images/rproxy-images/prox4_1.png
        - content: |
            The first container will exclusively host the API, and any URL with the /api path will be routed to this container. I am using a lightweight Python image for this container, along with a setup.py file to install the required dependencies. Additionally, I have defined an entry point in the application to launch the Uvicorn server using the <strong>"runserver"</strong> command. This application will run on port 8000 within the server.<br><br>
          imgs:

        - content: |

          imgs:
            - /assets/project-images/rproxy-images/prox4_2.png
            - /assets/project-images/rproxy-images/prox4_3.gif
        - content: |
            The Angular application runs in the second container and is served via Nginx, which also functions as the reverse proxy. In my Dockerfile, I copy the complete Nginx configuration into the container, along with the prebuilt Angular production files and the SSL certificates. Additionally, I configure file permissions within the container to ensure proper access to the SSL certificates, enabling the application to serve traffic securely over HTTPS. In the next section, I will provide a detailed overview of the Nginx configuration and explain how the reverse proxy is set up.<br><br>
          imgs:
            - /assets/project-images/rproxy-images/prox4_4.png

    - title: Reverse Proxy Configuration with Nginx
      tabTitle: Reverse Proxy
      subsections:
        - content: |
            The Nginx configuration is the backbone of this project. This is where the reverse proxy rules are defined to route traffic to the appropriate container based on the request path. To serve the API container through the /api path, I simply need to define a location block for each path.<br><br>

            The initial server block for port 80 is redundant in this setup because I define the SSL connection and container path locations in the configuration below. However, this block could serve as a fallback or facilitate HTTP communication between containers if needed in the future.<br><br>

          code: |
            error_log /var/log/nginx/error.log notice;

            server {
                listen       80;
                server_name  localhost;
                root         /usr/share/nginx/html;
                index        index.html;
                location / {
                        try_files $uri $uri/ /index.html;
                }

                error_page 404 /404.html;
                location = /404.html {
                }

                error_page 500 502 503 504 /50x.html;
                location = /50x.html {
                }
            }

          imgs:
        - content: |
            The main server block is defined here for the Angular container. In this block, I configure the server to use the SSL certificates created and validated in the previous step. The server listens on port 443, and under the location block, all traffic directed to the root path (/) is routed to the Angular application. Additionally, I define SSL protocols and ciphers, configure the session cache, and set the session timeout to ensure secure and efficient communication.<br><br>

          code: |
            <span style="color: rgb(0, 255, 0);">
            server {
                listen 443 ssl;
                server_name digitalsteve.net;

                ssl_certificate /etc/ssl/certs/fullchain.pem;
                ssl_certificate_key /etc/ssl/private/privkey.pem;

                ssl_session_cache shared:le_nginx_SSL:10m;
                ssl_session_timeout 1440m;
                ssl_session_tickets off;

                ssl_protocols TLSv1.2 TLSv1.3;
                ssl_prefer_server_ciphers off;

                location / {
                    root /usr/share/nginx/html;
                    index index.html;
                    try_files $uri $uri/ /index.html;
                }
              </span>


                location /api {
                    proxy_pass http://api-service:8000;
                    proxy_http_version 1.1;
                    proxy_set_header Upgrade $http_upgrade;
                    proxy_set_header Connection 'upgrade';
                    proxy_set_header Host $host;
                    proxy_cache_bypass $http_upgrade;
                }

            }
          imgs:
        - content: |
            The second location block is responsible for creating the reverse proxy. In this block, I define the /api path to route traffic to the API container. The <strong>proxy_pass</strong> directive specifies the container name and port where the API is running. Additional directives such as proxy_http_version, proxy_set_header, and proxy_cache_bypass are configured to ensure proper communication between the Nginx server and the API container. This block is the core of the reverse proxy setup, enabling me to serve both the Angular application and the API under the same domain.<br><br>

          code: |
            server {
                listen 443 ssl;
                server_name digitalsteve.net;

                ssl_certificate /etc/ssl/certs/fullchain.pem;
                ssl_certificate_key /etc/ssl/private/privkey.pem;

                ssl_session_cache shared:le_nginx_SSL:10m;
                ssl_session_timeout 1440m;
                ssl_session_tickets off;

                ssl_protocols TLSv1.2 TLSv1.3;
                ssl_prefer_server_ciphers off;

                location / {
                    root /usr/share/nginx/html;
                    index index.html;
                    try_files $uri $uri/ /index.html;
                }
            <span style="color: rgb(0, 255, 0);">

                location /api {
                    proxy_pass http://api-service:8000;
                    proxy_http_version 1.1;
                    proxy_set_header Upgrade $http_upgrade;
                    proxy_set_header Connection 'upgrade';
                    proxy_set_header Host $host;
                    proxy_cache_bypass $http_upgrade;
                }

            }
            </span>

          imgs:

    - title: Bash Scripting and Automation for Jenkins
      tabTitle: Bash Scripting
      subsections:
        - content: |
            Now that I have configured the reverse proxy, I can create Bash scripts to instruct Jenkins on deploying the entire setup to the server. I use a Jenkinsfile to define the pipeline job and outline the stages Jenkins will execute to deploy the applications. In the final stages, once the infrastructure is provisioned, Jenkins runs these scripts to handle the bulk of the deployment process.<br><br>

            The first script acts as an SSH utility, connecting to the server and copying the deployment files over SCP. To expedite the process, I bundle the production files for the Angular application locally rather than building them directly on the server. This decision, based on preference, has a minimal impact on deployment time given the application's size.<br><br>

            Once the deployment files are transferred, the script initiates an SSH connection to the instance and executes the next script in the deployment process.<br><br>
          imgs:
            - /assets/project-images/rproxy-images/prox6_1.png
        - content: |
            With the 'copy' script successfully transferring all necessary files to the server, the next step is to build and run the Docker containers that will host the applications.<br><br>

            <strong>In the first image below</strong>, the script installs the required dependencies on the server and navigates to the working directory. For this project, I parameterized the Jenkins job to support three options: Deploy, Update, and Destroy. The Deploy option initiates a fresh install, tearing down all existing AWS resources associated with the Terraform project and rebuilding everything from scratch. The Update option retains existing AWS resources while updating the application with the latest deployment files. To ensure a clean deployment, I included a check to stop and remove any running containers before proceeding with the Docker build and run commands.<br><br>

            Additionally, the Docker network is configured to ensure seamless communication between the containers, which is critical for the reverse proxy to function properly.<br><br>

            <strong>In the second image below</strong>, the script handles the building and running of both application containers. Since the Nginx configuration relies on the API container being operational, the API must be deployed first. To account for this, I added a sleep command to allow the container enough time to start up and for the network to recognize the running container. The run commands are as follows:<br><br>

          code: |

            docker build -t api:latest .
            <span style="color: blue;"># the container is built and tagged with the latest tag for the API image</span>

            docker run -d --network $NETWORK_NAME --name api-service -p 8000:8000 api:latest
            <span style="color: blue;"># the container is then run in detached mode, attached to the custom network, named and bound to the host machine on port 8000</span>


            docker build -t websitev2:latest .
            <span style="color: blue;"># the container is built and tagged with the latest tag for the Angular image</span>

            docker run -d --network $NETWORK_NAME --name website-service -p 443:443 websitev2:latest
            <span style="color: blue;"># the container is then run in detached mode, attached to the custom network, named and bound to the host machine on port 443</span>
          imgs:

        - content: |
            The script will then do some basic logging to make sure that the containers are running and that the deployment was successful.

          imgs:
            - /assets/project-images/rproxy-images/prox6_2.png
            - /assets/project-images/rproxy-images/prox6_3.png

    - title: Jenkins Configuration and Parameters
      tabTitle: Jenkins
      subsections:
        - content: |
            With the project scripts defined, I can now configure the Jenkins pipeline job to provision the infrastructure and deploy the applications. The pipeline stages required for deployment are specified within the Jenkinsfile.<br><br>

            The process begins by defining the environment variables that will be passed to the Terraform configuration file. These variables are crucial for ensuring the Terraform deployment aligns with the desired infrastructure setup. Next, the pipeline assumes the role for the AWS account where the applications are being deployed. This ensures that Terraform has the necessary permissions to provision the infrastructure.<br><br>

            If the pipeline is tasked with tearing down existing infrastructure, the 'Destroy' step is executed first. This step removes any resources tracked in the state file stored in S3/DynamoDB. If no teardown is required, the 'Destroy' step is skipped, allowing the pipeline to proceed directly to provisioning or updating the infrastructure.<br><br>
          imgs:
            - /assets/project-images/rproxy-images/prox7_1.png
            - /assets/project-images/rproxy-images/prox7_2.png

        - content: |
            After assuming the role and initializing Terraform, the next step involves referencing the plan saved in the state file. This step is critical for provisioning new infrastructure, updating existing resources, or simply verifying the current infrastructure's configuration. Once this process is complete, the Jenkinsfile proceeds to execute the initial 'copy' script. This script is responsible for transferring the necessary deployment files to the server, ensuring that all required assets are in place for subsequent steps.<br><br>

          imgs:
            - /assets/project-images/rproxy-images/prox7_3.png
            - /assets/project-images/rproxy-images/prox7_4.png

    - title: Configuring the Pipeline Job
      tabTitle: Pipeline
      subsections:
        - content: |
            This Jenkins project is configured as a straightforward pipeline job. The only required inputs are the GitHub repository URL and the location of the Jenkinsfile within the build directory. Once provided, the pipeline executes the stages as defined in the Jenkinsfile. To offer flexibility, the project is parameterized with a choice parameter (<strong>Deploy, Update, and Destroy</strong>), enabling the pipeline to be run in various modes depending on the desired outcome.<br><br>

            <strong>Note:</strong> The provisioned build server is designed to pull credentials from the IMDSv2 role attached to the instance during startup. This setup ensures that Terraform can create resources and Jenkins can assume roles as needed, provided the appropriate <strong>passrole</strong> and <strong>assumerole</strong> permissions are in place.<br><br>

          imgs:
            - /assets/project-images/rproxy-images/prox8_1.png
            - /assets/project-images/rproxy-images/prox8_2.png

        - content: |

          imgs:
            - /assets/project-images/rproxy-images/prox8_3.png

    - title: Running the Deployent Pipeline Job
      tabTitle: Deployment
      subsections:
        - content: |
            With the pipeline configured and the Jenkinsfile finalized, I can now execute the job to deploy the site and configure the reverse proxy for my API. The process begins with Terraform analyzing the infrastructure and reviewing the state files to determine if any resources need to be provisioned or updated.<br><br>
          imgs:
            - /assets/project-images/rproxy-images/prox9_1.gif
            - /assets/project-images/rproxy-images/prox9_2.png

        - content: |
            Following the infrastructure check, the deployment files are transferred to the server. This includes all necessary files such as the Dockerfiles and the Nginx configuration.<br><br>
          imgs:
            - /assets/project-images/rproxy-images/prox9_3.png
            - /assets/project-images/rproxy-images/prox9_4.png

        - content: |
            The Docker containers are built and executed on the server, starting with the API container since it is a dependency for the Nginx configuration. Once the API container is running, the Nginx container is built and launched with the reverse proxy configuration.<br><br>

            The complete pipeline view is displayed below, showing all the stages executed in the deployment process. After the pipeline finishes, deployment files are cleaned up from the build server, ensuring a clean slate for future builds. The pipeline process is now complete.<br><br>
          imgs:
            - /assets/project-images/rproxy-images/prox9_5.png
            - /assets/project-images/rproxy-images/prox9_6.png

    - title: Summary
      tabTitle: Summary
      subsections:
        - content: |

          imgs:
            - /assets/project-images/rproxy-images/prox_sum.png

        - content: |
            This project was initially conceived as an opportunity to integrate my API into my existing website while minimizing costs. Adding new AWS resources to run the application incurs significant overhead, including expenses for Load Balancing, CloudFront to utilize ACM (AWS Certificate Manager) certificates, and ECS (either Fargate or EC2 mode) for container management. Given that this is a lightweight site with a relatively small API, implementing a reverse proxy was a cost-effective solution. Managing these aspects independently allowed me to reduce costs significantly compared to provisioning additional infrastructure solely to host the API.<br><br>

            Before undertaking this project, I had limited experience with setting up reverse proxies or creating and managing domain-validated SSL certificates. This project provided an excellent learning opportunity to understand the various advantages of using a reverse proxy. It served as a gateway into the world of Nginx and server configurations, revealing the extensive customization options available. Reverse proxies offer numerous benefits, including load balancing, enhanced security, SSL termination, caching, monitoring, and loggingâ€”all of which make them a valuable design pattern.<br><br>

            I also explored different design patterns for implementing reverse proxies. For instance, I considered using a dedicated container for the reverse proxy versus embedding it within the main container. The choice depended on the application's complexity and specific use case. For this project, simplicity prevailed, and I opted to include the reverse proxy within the Angular application container.<br><br>

            The outcome of this project has been highly satisfying, both from a learning and cost-saving perspective. I can now run my API on the same domain as my website while leaving room to integrate additional applications as future projects grow in complexity. I also plan to experiment further with the possibilities that reverse proxies enable. Additionally, I gained a solid understanding of creating and managing SSL certificates, which I can leverage in upcoming projects. Overall, this project was a rewarding experience, and I am eager to apply the knowledge I gained in future endeavors.<br><br>
          imgs:

- id: jenkins
  title: Jenkins Pipeline - Dynamic Cloud Node Provisioning with AWS
  subtitle: Fully automated build pipeline with dynamically provisioned cloud node to push containterized applications to AWS.
  github: https://github.com/syuhas/jenkinsTest
  description: |
    Utilizing Jenkins to build and deploy Docker images to AWS ECS using dynamically provisioned EC2 build nodes to save cost on idle servers.
  listIcon: /assets/project-images/jenkins-images/jenkins_icon.png
  titleIcons:
    - /assets/project-images/jenkins-images/jenkins_icon.png
    - /assets/project-images/jenkins-images/jenkins_icon3.png
    - /assets/project-images/jenkins-images/jenkins_icon2.png
  sections:
    - title: Overview
      tabTitle: Overview
      subsections:
        - content: |

          imgs:
            - /assets/project-images/jenkins-images/jenkinsDocker.png

        - content: |
            For this project I wanted to delve into the popular CI/CD pipeline tool Jenkins and explore the benefits of using cloud computing to save on cost and resource usage. I created a simple Python Flask skeleton application to be containerized on Docker and my goal was to have Jenkins fully automate the process from the initial git push from my local machine all the way up to building and pushing the image to AWS ECR to be run as a task on ECS. To fully take advantage of the agility and speed of cloud computing as well as the â€˜pay as you goâ€™ philosophy behind cost savings, I wanted to only have a build server provisioned when I needed it. This way I am not running unnecessary idle servers, I can scale to multiple build nodes as needed in minutes and can benefit on the cost savings of only using the resources I need, when I need them. <br><br>

            Much of the challenge of a project like this is configuration of authorization and integration between services. This task is made much easier through the use of Jenkins plugins that allow for easier storing and usage of credentials. In this project review I will go over all of the configuration needed to automate the pipeline and highlight all of the features I took advantage of to achieve the end result.
          imgs:
    - title: Jenkins Master and Plugins
      tabTitle: Jenkins Master
      subsections:
        - content: |
            To run and configure all of the build jobs, I set up a master Jenkins server on an AWS EC2 instance. The server is routed through my domain at digitalsteve.net and behind a load balancer for HTTPS encryption and an Elastic IP address.
          imgs:
            - /assets/project-images/jenkins-images/master1.png
        - content: |
            The main plugins used for this project are:
          listItems:
            - Amazon EC2
            - Cloudbees AWS Credentials
            - Credentials
            - GitHub
            - Pipeline
            - SSH Credentials
          imgs:
            - /assets/project-images/jenkins-images/master2.png
            - /assets/project-images/jenkins-images/master3.png
    - title: GitHub Integration
      tabTitle: GitHub
      subsections:
        - content: |
            The first step of GitHub integration requires enabling a PAT(Personal Access Token) to be used on the Jenkins master server to allow Jenkins to use the GitHub API. This will allow Jenkins to clone my repository on my build node for the SCM checkout phase.
          imgs:
            - /assets/project-images/jenkins-images/git1.png
            - /assets/project-images/jenkins-images/git2.png
        - content: |
            Next, I set up a Webhook on GitHub that allows push notifications to flow downstream to Jenkins and trigger a pipeline job after my code updates are committed. Later, I will go over using the credentials when configuring a job and enabling the Webhook to push code from GitHub to Jenkins to trigger the pipeline to run.

            For the payload url I simply need to append /github-webhook/ to the end of my master server url to allow the Webhook to push code to Jenkins with the GitSCM polling trigger enabled.

            I also generated an API token for my username to secure the webhook to my account. This is then entered as the Webhook secret.
    - title: Python Application Configuration
      tabTitle: Python App
      subsections:
        - content: |
            I chose to use a simple Python Flask skeleton application to run through the pipeline to test out the functionality for demo purposes. The application will be run on a Docker container after building the image on the Jenkins build server and pushing the image to ECR to run as an ECS task.
          imgs:
            - /assets/project-images/jenkins-images/py1.png
            - /assets/project-images/jenkins-images/py2.png
        - content: |

          imgs:
            - /assets/project-images/jenkins-images/py3.png
            - /assets/project-images/jenkins-images/py4.png
    - title: AWS Integration
      tabTitle: AWS
      subsections:
        - content: |
            Correct AWS integration is the most important portion of the setup for this pipeline. Jenkins needs to be able to interact with AWS via the AWS CLI to provision servers, SSH into the dynamically provisioned cloud server to manipulate the EC2 instance OS, and ultimately push the Docker image to ECR. This is also the stage where the 'least privilege' principle is important to keep in mind as sensitive credentials will be passed through the server. Here I am using the Cloudbees EC2 plugin to securely input public/Secret access keys and SSH key pair credentials. For this project, I am granting more privileges than necessary. In production, a privilege analysis should be done to determine the least amount of authorization to run the pipeline.<br><br>

            First, I generated public and private access keys for my user and entered the credentials via the Cloudbees plugin.<br><br>

            I also attached an AWS role to the credentials which will be used for AWS services provisioning and pushing images to ECR which is described below.
          imgs:
            - /assets/project-images/jenkins-images/aws1.png
            - /assets/project-images/jenkins-images/aws2.png
        - content: |
            This is the role used for the AWS credentials in Jenkins.
            I have attached custom policies to the role, allowing the provisioned EC2 instance to:
          listItems:
            - Get an STS authorization token for pushing the Docker image to ECR
            - Giving the slave node server access to EC2 functionality
            - Allowing the slave node server to update the ECS service with the latest image
          imgs:
            - /assets/project-images/jenkins-images/aws3.png
            - /assets/project-images/jenkins-images/aws4.png
        - content: |
            In addition to the access keys and attached policies, the slave node will need to SSH into the instance to update and configure the OS and dependencies. This will allow Jenkins to run the init script and pass the AWS credentials to the Jenkinsfile pipeline script.
          imgs:
            - /assets/project-images/jenkins-images/aws5.png
    - title: ECR Permissions
      tabTitle: Permissions
      subsections:
        - content: |
            In order to allow the newly provisioned node to push the new image to ECR, permissions must be attached to the instance profile role.
        - content: |
            This will define the role's permission set for performing actions on the ECR repository.
          imgs:
            - /assets/project-images/jenkins-images/ecr1.png
        - content: |
            For demo purposes, I gave the instance profile role full rights to the ECR service for testing, but I can go back and redefine the permissions set to grant only the needed permissions for the tasks required in the pipeline.
          imgs:
            - /assets/project-images/jenkins-images/ecr2.png
    - title: Dynamically Provisioned EC2 Worker Node
      tabTitle: Worker Node
      subsections:
        - content: |
            Now that I have all of the necessary credentials configured and the permissions and policies defined, I can configure a template for dynamically provisioning new slave build nodes. The benefit of having servers spin up only when I need them is that these build nodes will only exist when the pipeline runs. After the idle timeout, the server will be automatically decommissioned until I need to run the pipeline again. Configuring this automation upfront really shows the power of cloud computing from a cost-savings perspective. Now, I will only ever need to have one minimal master node running for my Jenkins server when no jobs are running.
        - content: |
            First, I will use the AWS access keys with the attached IAM instance profile role and SSH key pair to authorize Jenkins to connect to AWS and then SSH into the newly created EC2 instance.
          imgs:
            - /assets/project-images/jenkins-images/cloud1.png
            - /assets/project-images/jenkins-images/cloud2.png
        - content: |
            I then need to define an AMI ID to indicate what type of instance I would like for Jenkins to spin up on my behalf. I also need to indicate which region my availability zones my instance can be created in as well as an instance type and size. I have already configured a security group with the necessary ports exposed (22, 8080, 80, 443) to allow for necessary inbound and outbound traffic to flow through my instance. I then tell Jenkins to attach that security group to each new node.
          imgs:
            - /assets/project-images/jenkins-images/cloud3.png
            - /assets/project-images/jenkins-images/cloud4.png
        - content: |
            I can now indicate which root folder for Jenkins to use to set up a temporary workspace that will act as the base of operations where my repository will be cloned and the Docker image can be built from. I am using a Unix type AMI, and will be allowing an SSH connection on port 22.

            I can then define how this node should be used. Since I only want Jenkins to spin up these nodes when needed, I set the built-in node to operate at 0, and have indicated this node configuration to be used as much as possible. This means that only dynamically provisioned nodes will be used for any type of job.
          imgs:
            - /assets/project-images/jenkins-images/cloud5.png
            - /assets/project-images/jenkins-images/cloud6.png
        - content: |
            Any additional setup not indicated by the plugin settings needs to be configured with the use of an init script, which will contain important directions for the new instance.

            The init script will:
          listItems:
            - Update the OS
            - Install Docker and add the remote user to the Docker group
            - Create, change permissions for, and add the remote user to the new Jenkins root folder
            - Change permission of the Docker service to allow the remote user to run Docker without the sudo command
          imgs:
            - /assets/project-images/jenkins-images/cloud7.png
            - /assets/project-images/jenkins-images/cloud8.png
        - content: |
            Finally, I just need to configure a few small connection settings and I can now spin up a dynamically provisioned node only when I need it.
          imgs:
            - /assets/project-images/jenkins-images/cloud9.png
    - title: Jenkinsfile & Dockerfile
      tabTitle: Jenkins/Dockerfile
      subsections:
        - content: |
            For this application, I am using a Jenkinsfile to write my pipeline build script.

            In the script, the first stage will build and publish the Docker image to ECR. This is where the access keys, attached STS identity policy and ECS policy come into play. This will allow the script to get a temporary login password to push the new image to ECR.<br><br>

            Next, the script will list the images on the repository. This will confirm that the push to ECR was successful.<br><br>

            Once the new image has been pushed to the repository, the script will now update the ECS cluster to run the new task on the ECS service. The number of tasks can be changed based on the use case, but for demo purposes I have set my task value to 1.<br><br>

            Finally, the script will wipe all Docker images or running containers from the server, allowing for a clean build if I need to run builds in close proximity.
        - content: |
            This will define the role's permission set for performing actions on the ECR repository.
          imgs:
            - /assets/project-images/jenkins-images/jd1.png
        - content: |
            For my Dockerfile, I simply need to pull a base python Docker image, set my working directory, and then copy and install my dependencies as defined in my requirements file.<br><br>

            I now can run the Flask application on a Gunicorn server on port 80 to allow inbound HTTP traffic. As long as I configure the ECS service to expose port 80 from within Docker, I will be able to access the task from a browser to view my application.
          imgs:
            - /assets/project-images/jenkins-images/jd2.png
    - title: Pipeline Job Configuration
      tabTitle: Pipeline
      subsections:
        - content: |
            With all of the scripts and code written and all of the authentication and configuration finished, I can now set up a new pipeline job to actually run the pipeline. I will be choosing a single branch pipeline for the purposes of this project as I will not need to be managing multiple build branches for such a simple application.
          imgs:
            - /assets/project-images/jenkins-images/pipe1.png
        - content: |
            The goal is to trigger the pipeline when I commit new code to my remote repository. I can now use the GitHub Webhook and Personal Access Token that I set up earlier to ensure this happens with every push I make to the repository. Once the pipeline is triggered from the Webhook, Jenkins will push the new code to each newly provisioned slave instance node. This will then allow Docker to build the image based on each new checkout from the repository.
          imgs:
            - /assets/project-images/jenkins-images/pipe2.png
            - /assets/project-images/jenkins-images/pipe3.png
        - content: |
            Jenkins will pull code from the main branch. I also need to specify the scripting that Jenkins will use. Here, I tell Jenkins to use the Jenkinsfile located at the base of my workspace directory. With all of the pieces in place, I will now able to run my pipeline which will use all of the automation to dynamically spin up a server to build my project.
          imgs:
            - /assets/project-images/jenkins-images/pipe4.png
    - title: Final Steps and Running the Pipeline
      tabTitle: Final Steps
      subsections:
        - content: |
            I can now run my pipeline automation. My pipeline will be triggered from my GitHubSCM Webhook. Jenkins will spin up a slave build node, build my Docker image, push the image to ECR, display the images on my repository, and finally update my ECS service with the latest image from the repository.<br><br>

            This can all be done by simply committing new code to my remote repository.<br><br>

            Jenkins will automate the entire process as soon as it receives the trigger.
          imgs:
            - /assets/project-images/jenkins-images/fin1.png
        - content: |
            Additionally, I can spin up a pre-configured slave node manually if I want to have the server running ahead of time. This is optional as the build server will be provisioned either way if I do not manually start the server.
          imgs:
            - /assets/project-images/jenkins-images/fin2.png
        - content: |
            Once the SCM Webhook is triggered, Jenkins will check for available build nodes. Since the previous node has been decommissioned, Jenkins will now tell AWS to spin up a fresh Jenkins server that is completely configured with credentials and instructions to be able to build and push my application.
          imgs:
            - /assets/project-images/jenkins-images/fin3.png
            - /assets/project-images/jenkins-images/fin4.png
        - content: |
            Here, we can see a bit of what is happening in the background. Jenkins is attempting to SSH into the newly created server and run the init script. The Jenkins node will be configured with its own Jenkins dependencies, and will then install the dependencies defined in the init script. Once the script is finished running, Jenkins will check to make sure the proper number of defined build nodes are online and then proceed to run the pipeline.
          imgs:
            - /assets/project-images/jenkins-images/fin5.png
            - /assets/project-images/jenkins-images/fin6.png
        - content: |
            Jenkins will now proceed to run the pipeline after it has a configured slave node up and running.
          imgs:
            - /assets/project-images/jenkins-images/fin7.png
            - /assets/project-images/jenkins-images/fin8.png
        - content: |
            The ECS service has been successfully updated and I can now connect to the newly created task in the service.
          imgs:
            - /assets/project-images/jenkins-images/fin9.png
            - /assets/project-images/jenkins-images/fin10.png
        - content: |
            To test that the ports were properly configured, I can simply visit the site in the browser through the task public DNS or public IP after the pipeline completes and the new service updates.
          imgs:
            - /assets/project-images/jenkins-images/fin11.png
        - content: |
            Now that the pipeline has run and my new task is running on ECS, an idle timeout will have the instance self-terminate. The idle timeout is 30 minutes so the same process will be repeated each time new code is committed outside of the idle window. Otherwise, Jenkins will use an existing node if it is still in service.
    - title: Summary
      tabTitle: Summary
      subsections:
        - content: |
            This project has made Jenkins my new favorite tool by far. It encompasses all the things that I love about cloud computing and DevOps. Integration, automation, containerization, dynamic provisioning of resources, and a â€˜pay-as-you-goâ€™ philosophy that makes cloud so innovative and interesting. I had a great time learning the ins and outs of Jenkins and the possibilities it presents with automating any task you can think of when paired with AWS.<br><br>

            My favorite aspect of this project was the automation for sure. When you do all the work up front and just watch it work, it really is that feeling of efficiency and preparation that makes CI/CD so interesting and fulfilling to me. I will be implementing Jenkins in more of my projects in the future and I have learned many useful tools while creating this project.<br><br>

            Integrating AWS and Jenkins really shows the power of cloud integration and automation and really inspires me to explore more of the possibilities that are present when combining powerful tools like this. I learned a lot doing this project and was very happy with the result. I look forward to hearing your reactions to the project and I sincerely hope you enjoyed this dive into the world of Jenkins.
          imgs:

- id: usermgmt
  title: User Management System
  subtitle: A Scalable Account Creation, Authorization and Management System for Web Application Users.
  github: https://github.com/syuhas/flaskdb
  description: |
    A functional user management system with registration, login authorization, database management and email validation.
  listIcon: /assets/project-images/usermgmt-images/usermgmt_icon.png
  titleIcons:
    - /assets/project-images/usermgmt-images/usermgmt_icon.png
    - /assets/project-images/usermgmt-images/usermgmt_icon2.png
    - /assets/project-images/usermgmt-images/usermgmt_icon3.png
  sections:
    - title: Overview
      tabTitle: Overview
      subsections:
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/usermgmt.png

        - content: |
            The purpose of this project is to build upon the basics of designing a web application within the Python Flask Framework by creating my own user Authorization and Management system for visitors to the site. Achieving this requires features like session management, database connection and modeling, email smtp servers, password encryption, and cloud storage. When the user visits the site, they are presented with a welcome page and given the ability to sign up or log in with a previously created account.<br><br>

            When signing up for a new account, input validation is performed, the user's password is encrypted using a hashing algorithm and their data is stored in an AWS MySQL Database. The user is then required to validate the email provided by clicking a time-sensitive, personalized link that is generated with a URL compatible serializer token. Upon completion of the email validation process, the user is now able to log in and set up their profile information including a profile picture that is uploaded and saved to Amazon S3. If at any point the user forgets their password, the option is given to send a password reset link to the user's email. They are also able to update their existing email by carrying out an additional email validation process, and can also choose to delete their account.<br><br>

            For the puropses of this project, the app is located in a different server environment. I will descibe all of the elements relevant to the user management system, while omitting the finer details of configuring an Elastic Beanstalk environment, setting up a pipeline, enabling SSL/TLS and enabling DNS routing. These steps can be found in my previous project found <a href="/projects/website" target="_blank">here</a>.
          imgs:

    - title: Design and Outline
      tabTitle: Design
      subsections:
        - content: |
            The user management consists of 3 main elements: a signup page, a login page, and a user profile page. In addition to this, the user can logout or reset their password as well as delete their account within the user profile. I will go over the design and functionality of each of these elements in the following sections.
          imgs:
            - /assets/project-images/usermgmt-images/design1.png
        - content: |
            All of the UX design for the user input pages have been designed with bootstrap for this project. The UX forms were made with the form-control bootstrap class.
          imgs:
            - /assets/project-images/usermgmt-images/design2.png
            - /assets/project-images/usermgmt-images/design3.png

    - title: Application Factory Context and Configuration
      tabTitle: App Factory
      subsections:
        - content: |
            An Application Factory structure is used for this project. This structure allows for more growth and complexity in the future. In order to make this work and avoid circular imports within the blueprints, the blueprints themselves are defined within an app context. The application creation function will then be imported and called to start the application.
          imgs:
            - /assets/project-images/usermgmt-images/app1.png
        - content: |
            Next, the application creation function is called which will initalize and run the application. During production operation, the debug feature will be disabled. For testing purposes it is useful to keep on to diagnose problems with the application.
          imgs:
            - /assets/project-images/usermgmt-images/app2.png
        - content: |
            The project structure is set up with a main app folder that contains the application factory, the blueprints, and the main application file. The blueprints are then imported into the application factory and the application is run. The templates  and static folders contatain the html and css files for the project and define how the application will look and feel.
          imgs:
            - /assets/project-images/usermgmt-images/app3.png

    - title: The Development Pipeline Structure and Workflow
      tabTitle: Pipeline
      subsections:
        - content: |
            Using GitHub, CodePipeline and Elastic Beanstalk, the workflow is automated in a pipeline from the IDE to the server environment. As the code is synced into the main branch of GitHub, CodePipeline will pull the code and automatically load the changes into the environment.
          imgs:
            - /assets/project-images/usermgmt-images/pipe1.png
            - /assets/project-images/usermgmt-images/pipe2.png
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/pipe3.png
            - /assets/project-images/usermgmt-images/pipe4.png

    - title: Environment Variables Configuration
      tabTitle: Env Variables
      subsections:
        - content: |
            Any sensitive information within the code such as private keys and database passwords needs to be masked through the use of environment variables since this code will be publicly visible on GitHub. Elastic Beanstalk allows for an easy way to store environment variables through the use of environment properties. In the local code base, a separate file is created to hold the values of the environment variables using the OS extension and are imported when needed.
          imgs:
            - /assets/project-images/usermgmt-images/env1.png
            - /assets/project-images/usermgmt-images/env2.png

    - title: Database Schema and Connection
      tabTitle: Database
      subsections:
        - content: |
            SQLAlchemy is used to define the database schema and communicate with the database for this project. I decided to use the SQLAlchemy ORM (Object Relational Mapper) to define the schema for the user class. The ORM is a library that facilitates communication between the database and Python. In simple terms, the ORM maps Python classes to tables within a relational database, in this case MySQL hosted on AWS. This way, I can use Python to communicate with the database and make SQL queries instead of using the SQL syntax, which simplifies things and makes for much cleaner code. The ORM will translate function calls into SQL statements and create a connection pool session each time I need to read from or write to the database.
          imgs:
            - /assets/project-images/usermgmt-images/db1.png
        - content: |
            Here the user schema is defined as a class with each column being defined along with it's data type. This is thanks to the use of the declarative base which is a metadata container that will map the class to the table. The ORM engine is first created using the dialect, credentials and location of the remote database. After the table is defined, the create all function is called during the initial configuration of the database. This will actually create the database table and all of the configuration defined using the engine. After the inital creation of the database is configured, the engine connection can now be called as a session each time changes need to be made or data needs to be retrieved.
          imgs:
            - /assets/project-images/usermgmt-images/db2.png
        - content: |
            Now that the database schema is defined and the database table is created, I can use the engine to connect to the database throughout the program. The function connect() will create a database session using sessionmaker which creates a new connection pool to the database. The function will then return that session which will be used to query the database. The session is a essentially an intermediary that holds the class object and must be bound to an engine. The session retains the object data until a transaction is commited to the database or the transaction is rolled back. Using this format allows for me to adapt the program to pass in different database tables as parameters to the connect() function. For this project, however, I only have one table and the bound engine will always be the user class.
          imgs:
            - /assets/project-images/usermgmt-images/db3.png
        - content: |
            Here is one example of the session being used to query the database. The connect() function is called and returns the session as the local_session variable. Arguments are passed to this function which creates a new unconfirmed user. The new user is defined using the User class. That new user is then added to the database session. Since the session is just a holding zone for the data, no changes will be made to the database until the session is actually commited. Once the session changes are commited to the database, the connection is automatically closed by the ORM and the session data is flushed.
          imgs:

    - title: Creating a New User
      tabTitle: Create User
      subsections:
        - content: |
            With all the pieces in place to connect to the database, users are now able to be created and added to the user table. In order to do this, I needed to program some logic to have the server recieve the POST request and then handle and validate the user input. Additionally, I need to have the program safely store the user's password and make sure that their email is valid. In later steps, I will show how the users email can be validated using tokens and how I am able to securely store passwords in the database using password hashing algoriothms.
        - content: |
            In the HTML for the signup page, the variables inside the form tags under the name attribute will be passed to the server as a POST request. The back end of the application will then recieve these variables to perform validation on them and pass them along to the next step to be processed and commited to the database.
          imgs:
            - /assets/project-images/usermgmt-images/createuser1.png
        - content: |
            Here, the application first checks if the request type is a POST or a GET request and will route the traffic accordingly. If the message recieved is a POST request, the application will extract the name attributes for each input, save them as Python variables, and perform input validation to make sure the email and passwords match and that the username and email are both unique. The username and email are checked by connecting to and querying the database, checking the email and username against each table row entry in the context of a 'for' loop, and then passing the data along to be confirmed by the user. Along the way, the password is also hashed, which I will go over in the next step.
          imgs:
            - /assets/project-images/usermgmt-images/createuser2.png

    - title: Password Hashing
      tabTitle: Hashing
      subsections:
        - content: |
            During the user validation process, the user's password is hashed using an algorithm provided by the Flask-bcrypt extension. This will allow the user passwords to be stored safely in the database as the hashed value rather than plain text. In addition to hashing, flask-bcrypt adds a salt to the password which adds another layer of complexity to the hashing process and thus makes the password more secure.
        - content: |
            Here I have two functions that perform the hashing and validation process. The first function will hash the user's password then return it as a hashed value to store to the database. The second function will be able to check the hashed version against the plain text version to see if they match. The number of rounds specified in the hashing function is known as the cost factor, which basically defines the level of complexity of the salting process. A higher number means that it takes more time to hash the value, therefore making brute force attacks harder to perform.
          imgs:
            - /assets/project-images/usermgmt-images/hash1.png
        - content: |
            I can now call either of these functions when I need to hash a password for a new user or check an existing password against the user input when they are logging in. Note that the user's password is at no point being stored in plain text past the point that it is sent to the hashing function. I will not have access to it after this initial process which means I cannot retrieve the password either. This is due to the fact that hashing, as opposed to encryption, is a one way process, meaning that when the user inputs their password, the program will never have access to the plain text version again. For this reason, a system needs to be in place to have the user reset their password since I can never retrieve it for them. This is done in a later step through the password recovery email.
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/hash2.png
            - /assets/project-images/usermgmt-images/hash3.png
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/hash4.png
        - content: |
            Now if I log into the database with a SQL editor tool, I can view the database entry that was created. The username is stored, a unique user ID is generated, and the password is stored as it's hashed value. One other entry is generated under the confirmed column which is initialized to 0. This is a bool value that will be used in the next step to track whether or not the account has been verified by the user via email.

    - title: SMTP Mail Server Configuration
      tabTitle: SMTP
      subsections:
        - content: |
            In order to have users confirm emails and change passwords, I needed an SMTP mail server. Gmail provides this service for use cases such as mine. I first needed to create a unique application password to connect this application to my account. I am now able to have this application send out emails on my behalf. Some minor configuration must be done in the program in order to do this.
        - content: |
            Here I am using the FlaskMail extension which allows for an SMTP server to be defined and initialized to be used within the application. Within the inital function that defines the application, I initialize the SMTP server with the correct settings to allow the application to send emails.
          imgs:
            - /assets/project-images/usermgmt-images/mail1.png
        - content: |
            Once initialized, I can use the server to send out emails on my behalf with a defined template and reciever address. This configuration will be used to set up the initial email verification, the account password reset feature, and the ability to have the user set a new email with additional verification in the future as desired.
          imgs:
            - /assets/project-images/usermgmt-images/mail2.png

    - title: Email Verification
      tabTitle: Email Verify
      subsections:
        - content: |
            At this point the I have set up the application to create a user, hash the user's password, and store the user and hashed password in the database to have the user log in. Now I need a way to make sure the user actually owns the email they signed up with. This way, if the user forgets their password, I can have the application perform a password reset via email because we both confirmed we know the user has access to the email on file. In order to due this I used a combination of the SMTP server I previously configured, and a flask extension called itsdangerous. In this extension I need a tool called the URLSafeTimedSerializer. This will allows me to generate a URL compatible token to create a link that the user can click on to confirm the password.
        - content: |
            Once the user signs up, the application will flash this message, alerting them that an email has been sent to the address they entered. They must click the link within the email to confirm their account, which sets the confirmed database column to 1. This signifies that the value is True, meaning the email has been confirmed. The user can now log in and set up their profile information.
          imgs:
            - /assets/project-images/usermgmt-images/validate1.png
        - content: |
            If the user tried to login before verifying their email, the application will flash a message indicating that they must confirm before proceeding. From this screen they may also send another email with a fresh link. Every link they recieve is actually on a time limit which is set to whatever value I choose (in this case 30 minutes), and after that the link will expire.
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/validate2.png
            - /assets/project-images/usermgmt-images/validate3.png
        - content: |
            Once the link is clicked, the user is taken back to the login screen where they can continue to the site to fill out their user profile. In the next section I'll cover the logic and details behind how the program is able to achieve this.

    - title: Verification Code Logic
      tabTitle: Logic
      subsections:
        - content: |
            In a previous section I covered how the user input is actually validated and how they are entered into the system. At the end of that route, a redirect is called to the mailer.send_confirm_email route in another file. This file handles all of the mail delivery for email verification and password resets, pictured in this section.
          imgs:
            - /assets/project-images/usermgmt-images/logic1.png
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/logic2.png
            - /assets/project-images/usermgmt-images/logic3.png
        - content: |
            At the beginning of this route, the configuration for the email that will be sent is defined. I have a simple HTML template that will display the email pictured above. The email needs to be combined with a token that is generated with the help of the URLSafeTimedSerializer extension from itsdangerous. The s.dumps will combine the email (and added salt for security) into a unique token that is a URL safe string. This is then attached to the link that will be sent to the user's email. The _external=True tag simply signifies that it is being sent as an absolute URL as opposed to a relative link within our web page. Basically, when the link is clicked it will redirect the user to the route mailer.confirm_email which contains the code that will actually change the database entry to confirmed=1, meaning they are now confirmed and ready to proceed with the login process.
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/logic4.png
            - /assets/project-images/usermgmt-images/logic5.png
        - content: |
            Here in the mailer.confirm_email route we can see where the link is taking the user. In this route we can also see that the token is sent in as an argument and as part of the URL. By doing this, I have created a unique route for each new token that is generated which then can be used to move ahead and confirm the user, since the token contains the user's email. Using s.loads, the token can be loaded back into it's original form before it was tokenized, used to query the database to find the user, and change the confirmed flag in the database to True. The user is then redirected to the login screen where they can successfully log in to the site. Notice also that the max_age for the token is defined here in the s.loads function, which defines the lifetime of the token. In this case, the token will expire after 30 minutes.
          imgs:
            - /assets/project-images/usermgmt-images/logic6.png

    - title: Login and Authorization
      tabTitle: Login
      subsections:
        - content: |
            So far, the application has taken in user input via a sign up table, validated the data, connected to a database where the new user is added, sent the user an email verification link to confirm their account. The user is then returned to the login screen with a message letting them know that the confirmation was a success. Now the user is able to log in since they have the confirmed flag set to 1 (True) in the database. Most of the heavy lifting is out of the way and I now simply need to verify the user exists in the system when they go to log in and that their password matches the stored value in the database by sending the password to the hash check function.

        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/login1.png
        - content: |
            As a separate exercise within the project, I decided to test out the FlaskForm extension from flask-wtf to recreate one of the forms in this project. This extension is a more elegant way of creating forms in flask. I simply needed to define my form as a class and import that class to be used for the login form. This essentially produces the same result as manually creating forms and handling POST requests, but with the added benefits of cleaner code and built-in input validation. It is a great tool that can definitely simplify the form creation and coding process.
          imgs:
            - /assets/project-images/usermgmt-images/login2.png
        - content: |
            Now the application performs basic input validation via FlaskForm to make sure the user inputs some data. To actually check the data against the database, the program calls the user route.
          imgs:
            - /assets/project-images/usermgmt-images/login3.png
        - content: |
            At the user route, the application connects to the database to first check if the user actually exists and that the user has correctly entered their username. Once the username is verified, the user's password is sent to the hash check function. This will return either True or False to check for a valid password. Once the password is verified, the program will check that this is a confirmed user via email verification. If all of these checks pass, the user is logged in and can proceed to the final step of filling out a profile.
          imgs:
            - /assets/project-images/usermgmt-images/login4.png
        - content: |
            If at any point the user decides to log out, they are sent to the logout route which will remove them from the session and return them to the login screen yet again.
          imgs:
            - /assets/project-images/usermgmt-images/login5.png

    - title: User Profile
      tabTitle: Profile
      subsections:
        - content: |
            For the final section, I wanted to have the user actually be able to do something now that they are logged in. I created a simple user profile where the user can fill out some basic contact information and use a delete feature if they wish to delete their account. As an added extra, the user can add a profile picture to their profile, made possible with the use of AWS S3 for hosting the pictures on the back end. All I needed to do was configure the proper permissions for the application to access my S3 bucket, which is served via HTTPS along with the rest of the site, as well as configuring some minor nginx file upload limits on the EC2 server for server to accept larger files for profile picture uploads.

        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/prof1.png
            - /assets/project-images/usermgmt-images/prof2.png
        - content: |
            When greeted at the profile screen, the user has the option of filling in any of their profile details or profile image and updating with one click. The changes will be reflected upon being redirected back to the page, and messages will flash on the screen confirming that their requested details were successfully updated.
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/prof3.png
            - /assets/project-images/usermgmt-images/prof4.png
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/prof5.png
        - content: |
            The user also has the option of updating their password or updating their email. Updating the email will automatically set the user to unconfirmed and send the user another token link. They will follow the same validation process to validate the new email as when they initially created their account. If the user chooses to delete their account, a warning modal will pop up on the screen where some simple Javascript will require them to enter their username before the final option to delete is available.
        - content: |

          imgs:
            - /assets/project-images/usermgmt-images/prof6.png
            - /assets/project-images/usermgmt-images/prof7.png
        - content: |
            The user details are placed into a 'context' for the user and sent to the user profile template as a keyword argument where they can be unloaded in the template and displayed in their applicable forms as placeholders. This is done simply to clean up the code a bit, and allow for only one argument to be sent to the template.
          imgs:
            - /assets/project-images/usermgmt-images/prof8.png
        - content: |
            The user details are placed into a 'context' for the user and sent to the user profile template as a keyword argument where they can be unloaded in the template and displayed in their applicable forms as placeholders. This is done simply to clean up the code a bit, and allow for only one argument to be sent to the template.
          imgs:
            - /assets/project-images/usermgmt-images/prof9.png
            - /assets/project-images/usermgmt-images/prof10.png
        - content: |
            Now the only step left is to connect and upload the profile picture to AWS S3. This is done through the us of Boto3, the AWS SDK for python. I made a few configurations to load the file onto my S3 bucket as the name of the user followed by "_profile_img" so they are easier to track in the bucket and will be overriden in case the user changes their picture again. In addition, the bucket is also versioned so older versions of their profile pictures are kept for a given amount of time before they are permanently deleted.<br><br>

            I also need a way to connect each user's picture to their profile beyond the naming scheme of the files. In order to acheieve this, I have the program save the URL location of the bucket object as an entry in the database for each user. That way, I can display the user's profile image by simply querying the database and loading the stored S3 URL, along with the rest of the user context, into the user profile template.
          imgs:
            - /assets/project-images/usermgmt-images/prof11.png
            - /assets/project-images/usermgmt-images/prof12.png

    - title: Summary
      tabTitle: Summary
      subsections:
        - content: |
            My biggest takeaway from this project was that looks are often deceiving. A simple looking web concept on the surface can have layers of complexity under the hood. There are many more moving parts going on here than I would have thought given a cursory glance. The great thing for me about coding and developing projects like this are the moments where a complex series of operations going on behind the scenes produces a simple and clean finished product. Projects like these require lots of new learning experiences and chances to try out new technologies while producing something people can interact with, making this a very enjoyable project to have undertaken.<br><br>

            I picked up many useful skills progressing through this project. From database management to cloud services integration all the way to handling more complex POST requests. I enjoyed learning more about how session data works and how it is handled in different contexts ranging from server side temporary data to connection pooling with a relational database. Learning through mistakes made along the way was a great way to gaining insight about how these systems can be optimized and improved as I create more complex projects in the future.<br><br>

            All around this was an invaluable project to me in my progression towards learning more about cloud and web infrastructure and I will use many of these tools in future projects moving forward. Thanks for taking the time to review this project and please reach out with any comments or suggestions.<br><br>
          imgs:

- id: websitev1
  title: Personal Portfolio Website (V1)
  subtitle: How I Built My First Website Using various AWS Services and Python Flask.
  github: https://github.com/syuhas/web_resume
  description: |
    The first version of my personal website. Built using Flask, HTML, and various AWS services, this project laid the foundation for showcasing my skills and projects, and has since been redesigned using Angular.
  listIcon: /assets/project-images/websitev1-images/site_icon2.png
  titleIcons:
    - /assets/project-images/websitev1-images/site_icon.png
    - /assets/project-images/websitev1-images/site_icon2.png
    - /assets/project-images/websitev1-images/aws.png
  sections:
    - title: Overview
      tabTitle: Overview
      subsections:
        - content: |

          imgs:
            - /assets/project-images/websitev1-images/WebsiteDiagram.png
        - content: |
            I created this site as a first project when transitioning into the industry. This was the first version of the current version of this website, and it was built from the ground up using AWS for features like hosting, CI/CD aspects, DNS services, load balancing, caching and TLS authentication. I have coded all the Python, HTML and CSS via a Flask app which is lightweight and provides the flexibility to add features in the future as desired. This project was not only a great way to display all of the content and projects I have completed but also an effective way to highlight some of the many capabilities of AWS when it comes to building a website or web app.<br><br> 

            In addition, this was a fantastic learning experience to pull together all of the things I have learned so far in AWS and coding. In this overview of the site, I will review all of the core pieces of its development. As I implement more features, I will document them as separate projects to track the evolution of the site.
          imgs:

    - title: Python Flask / HTML / CSS
      tabTitle: Flask
      subsections:
        - content: |
            Starting with a simple Python Flask app, I laid the necessary groundwork needed for the basic outline of the site to be built, including the blueprint that will link to all of the html templates to each route on the base HTML template.
          imgs:
            - /assets/project-images/websitev1-images/site-python.png
        - content: |
            This allowed me to write the base HTML template, which serves as the basic design of the site, including the navigation bar, footer and background design.<br><br>

            Bootstrap was used for a navigation bar and the layout of the site was configured. This provided a working skeleton to which additional pages can be added. Jinja is used for the templating in Flask to extend the additional HTML from this base page.
          imgs:
            - /assets/project-images/websitev1-images/site-html.png
        - content: |
            Using an external CSS stylesheet, the look of the site can be completely controlled in one place. A combination of Grid template areas and FlexBox was used so the layout of the site can be further fine-tuned to allow for things like media queries for viewing on different devices. This is where the bulk of the design for the site takes place.
          imgs:
            - /assets/project-images/websitev1-images/site-css.png
        - content: |
            This is the basic concept for the front end portion of the site. In addition to using all of these elements, the site was developed using a local Virtual Environment so only the necessary dependencies are installed and utilized as needed. This makes things easier when creating and uploading a requirements text file to be used with Elastic Beanstalk in the future, and ensures that there are no unnecessary dependencies.

    - title: Github
      tabTitle: GitHub
      subsections:
        - content: |
            In order to sync with to the build pipeline and to implement version control, I used GitHub for a git repository. After creation, the local files were connected and synced with the GitHub repository.
          imgs:
            - /assets/project-images/websitev1-images/site-git.png
        - content: |
            Now git commands can be used to upload changes to the repository. At the pipeline stage, these changes can then be sent to build, test and deploy stages after they are synced directly the AWS environment from the repository.
          imgs:
            - /assets/project-images/websitev1-images/site-gitcmnds.png

    - title: Elastic Beanstalk
      tabTitle: Beanstalk
      subsections:
        - content: |
            Once the Github repository was set up, I needed a place to host the Flask application. Elastic Beanstalk is a one-view, code-centric way of hosting the site on AWS. Beanstalk makes use of templating via CloudFormation , and storing of code via S3 , so I am able to upload the code to the site manually or via a pipeline. In addition, it uses containerization on EC2, which supports many languages (Python in this case) and it also takes care of instance configuration, OS, health checks, and configuring load balancing behind the scenes. All that needs to be done is to make sure that the Elastic Beanstalk CLI is installed on the virtual environment I'm using so that the configuration can be added when syncing to the pipeline and that the proper dependencies are installed and added to a requirements.txt file, providing Beanstalk the information for what to deploy on the server.
        - content: |
            First the Elastic Beanstalk CLI and dependencies were manually installed to the virtual environment via pip.
          imgs:
            - /assets/project-images/websitev1-images/site-eb-1.png
        - content: |
            In addition, Beanstalk requires adding a Requirements.txt file to ensure that the proper dependencies are installed on the server. This will be uploaded with the code to the environment.
          imgs:
            - /assets/project-images/websitev1-images/site-eb-2.png
        - content: |
            At this point the Elastic Beanstalk environment could be configured. A web server environment was selected.
          imgs:
            - /assets/project-images/websitev1-images/site-eb-3.png
        - content: |
            Flask runs on Python which is the AWS managed platform being used for this application.
          imgs:
            - /assets/project-images/websitev1-images/site-eb-4.png
        - content: |
            Once the environment was configured and deployed, any changes made to the code via our git repository will flow through the pipeline to the environment and be reflected on the site.
          imgs:
            - /assets/project-images/websitev1-images/site-eb-5.png

    - title: CodePipeline
      tabTitle: Pipeline
      subsections:
        - content: |
            For this step, the Codepipeline aspect of the site was configured. Codebuild and CodeDeploy steps will be added to the pipeline as the site evolves and more features are added. This will allow for more frequent changes to be made and tested without sending the changes directly to the production environment.
        - content: |
            For this step, the Codepipeline aspect of the site was configured. Codebuild and CodeDeploy steps will be added to the pipeline as the site evolves and more features are added. This will allow for more frequent changes to be made and tested without sending the changes directly to the production environment.
          imgs:
            - /assets/project-images/websitev1-images/site-pipelinesetup-1.png
        - content: |
            At the source configuration stage, a source of changes made to the code needs to be selected. Since the site uses GitHub for the repository, this will be the code source for the pipeline. Version 2 is the recommended version for this application.
          imgs:
            - /assets/project-images/websitev1-images/site-pipelinesetup-2.png
        - content: |
            At this point the GitHub repository was connected to the pipeline and ready to be synced to the desired provider.
          imgs:
            - /assets/project-images/websitev1-images/site-pipelinesetup-3.png
        - content: |
            In the deploy stage, a service is selected for where the code will sync to. Since Elastic Beanstalk is how I am deploying the site, it is selected as the provider. Now any changes made will go through the pipeline and deploy to the Elastic Beanstalk environment.
          imgs:
            - /assets/project-images/websitev1-images/site-pipelinesetup-4.png
        - content: |
            Once the pipeline is configured, I can see the source and deploy stages as they progress. The Elastic Beanstalk environment is now setup to receive changes through the pipeline.
          imgs:
            - /assets/project-images/websitev1-images/site-pipeline-deploy.png

    - title: SSL/TLS Certificate
      tabTitle: SSL/TLS
      subsections:
        - content: |
            I chose to distribute and cache the content on this site via CloudFront. Before doing that, I wanted to needed to make the site secure using SSL/TLS authentication so the site can be connected to securely via HTTPS. To do this, I had to request a public certificate for DNS validation which can be done through the AWS Certificate Manager (ACM). The certificate can then be deployed to any relevant resource I choose.
        - content: |
            First, the certificate was requested for DNS validation with the fully qualified domain name.
          imgs:
            - /assets/project-images/websitev1-images/site-cm-1.png
        - content: |
            At this point, the certificate was ready to be deployed on CloudFront.
          imgs:
            - /assets/project-images/websitev1-images/site-cm-2.png
        - content: |
            Later when the certificate is deployed, it can be viewed on the site from the CloudFront domain.
          imgs:
            - /assets/project-images/websitev1-images/site-cm-3.png

    - title: Web Application Firewall
      tabTitle: WAF
      subsections:
        - content: |
            Another optional component that can be added before the CloudFront distribution is created is a Web ACL, which is a stateless firewall for an added layer of protection for the site.
        - content: |
            To requisition an ACL using AWS WAF, I simply needed to choose a name and resource type. In this case a CloudFront distribution is the resource type.
          imgs:
            - /assets/project-images/websitev1-images/site-waf-1.png
        - content: |
            ACL rules can now be configured moving forward.
          imgs:
            - /assets/project-images/websitev1-images/site-waf-2.png
        - content: |
            Now the web ACL was available to be associated with the CloudFront distribution.
          imgs:
            - /assets/project-images/websitev1-images/site-waf-3.png

    - title: CloudFront
      tabTitle: CloudFront
      subsections:
        - content: |
            With the WAF ACL and SSL/TLS certificates set up, the CloudFront distribution could be created. CloudFront is a way to cache content at AWS edge locations so the content is served from those edge locations which reduces the load on the origin server.
        - content: |
            To create a distribution, an origin domain must be selected, in this case it will be the previously configured Elastic Beanstalk domain.
          imgs:
            - /assets/project-images/websitev1-images/site-cf-1.png
        - content: |
            For the default cache behavior, the default path pattern was used meaning all files will be cached on the site. This cannot be changed after creation, but if required it could be determined which types of files or directories will be cached.<br><br>

            For the viewer protocol policy, I want to direct HTTP to HTTPS since I've previously configured the SSL/TLS certificate for the site.<br><br>

            The cache policy will be a caching optimized policy. This is set to a TTL of 86,400 seconds(one day) with a maximum of one year.
          imgs:
            - /assets/project-images/websitev1-images/site-cf-2.png
        - content: |
            Next, a price class was selected. I chose to use all edge locations but the option to limit to certain regions is available. This can lower costs for the service.<br><br>

            Now the Web ACL that was previously confgured can be added to the distribution as well.
          imgs:
            - /assets/project-images/websitev1-images/site-cf-3.png
        - content: |
            Additionaly, the SSL/TLS certificate that we requested previously can now be deployed onto the distribution.
          imgs:
            - /assets/project-images/websitev1-images/site-cf-4.png
        - content: |
            The CloudFront distribution was then created. At this step, I now have a fully operational distribution with a firewall, which can be visited securely over HTTPS port 443 via the SSL/TLS certificate.
          imgs:
            - /assets/project-images/websitev1-images/site-cf-5.png

    - title: Route53
      tabTitle: Route53
      subsections:
        - content: |
            The final aspect to the site was setting up the DNS services with Route53. With Route53, a domain name can be purchased, creating a hosted zone followed by records to route all traffic visiting the domain directly to the CloudFront distribution.
        - content: |
            To purchase a domain name through Route53 I simply needed to enter the desired name and check that it was available. In this case I chose www.DigitalSteve.net as an example for this project.
          imgs:
            - /assets/project-images/websitev1-images/site-r53-1.png
        - content: |
            The domain name will now show as available for use under registered domains.
          imgs:
            - /assets/project-images/websitev1-images/site-r53-2.png
        - content: |
            A hosted zone then had to be created for the domain name so records can be added to route traffic visiting the domain.
          imgs:
            - /assets/project-images/websitev1-images/site-r53-3.png
        - content: |
            Finally, records were able to be added to the hosted zone to route traffic from the domain names to a destination. Two records were added, one for the domain name without the subdomain and one for the fully qualified domain name. For the routing, an Alias A Record was used to route the domain directly to an AWS service, that service being the CloudFront distribution for both records.
          imgs:
            - /assets/project-images/websitev1-images/site-r53-4.png
            - /assets/project-images/websitev1-images/site-r53-5.png

    - title: Summary
      tabTitle: Overview
      subsections:
        - content: |

          imgs:
            - /assets/project-images/websitev1-images/site-outro-1.png
        - content: |
            My biggest takeaway from this project was that looks are often deceiving. A simple looking web concept on the surface can have layers of complexity under the hood. There are many more moving parts going on here than I would have thought given a cursory glance. The great thing for me about coding and developing projects like this are the moments where a complex series of operations going on behind the scenes produces a simple and clean finished product. Projects like these require lots of new learning experiences and chances to try out new technologies while producing something people can interact with, making this a very enjoyable project to have undertaken.<br><br>

            I picked up many useful skills progressing through this project. From database management to cloud services integration all the way to handling more complex POST requests. I enjoyed learning more about how session data works and how it is handled in different contexts ranging from server side temporary data to connection pooling with a relational database. Learning through mistakes made along the way was a great way to gaining insight about how these systems can be optimized and improved as I create more complex projects in the future.<br><br>

            All around this was an invaluable project to me in my progression towards learning more about cloud and web infrastructure and I will use many of these tools in future projects moving forward. Thanks for taking the time to review this project and please reach out with any comments or suggestions.<br><br>
          imgs:

- id: cdk
  title: AWS CDK Stack Creation
  subtitle: "AWS Cloud Development Kit: Infrastructure as Code with Typescript"
  github: https://github.com/syuhas/cdk-demo-vpc
  description: |
    Defining and provisioning AWS resources as code with the AWS Cloud Development Kit (CDK).
  listIcon: /assets/project-images/cdk-images/cdk_icon.png
  titleIcons:
    - /assets/project-images/cdk-images/cdk_icon.png
    - /assets/project-images/cdk-images/cdk_icon2.png
  sections:
    - title: Overview
      tabTitle: Overview
      subsections:
        - content: |

          imgs:
            - /assets/project-images/cdk-images/CDKDemo.png
        - content: |
            This project illustrates the installation and operation of the AWS CDK(Cloud Development Kit), an IDE-based framework that lets you define infrastructure as code (IaC) using CloudFormation to provision the resources. An alternative to using the CloudFormation templates themselves, the CDK actually auto-generates the templates based on the code you define in one of many supported lanuages. This is a great solution for those who prefer to use a programming language of their choice to define and provision AWS infrastructure. For the following exercise, I will go over utilizing the CDK to provision a simple VPC with 3 public facing subnets, in 3 separate availability zones.
          imgs:

    - title: Installing Typescript and the CDK
      tabTitle: Installation
      subsections:
        - content: |
            First, there are some prerequisties that need to be in place before actually installing the CDK. As a preliminary measure, the AWS CLI must be installed and configured with the account credentials for the IAM role being used. Node.js is also required to use the CDK, so that must be installed beforehand. Here I will be installing Typescript and the CDK itself. In addition to this, things like Javascript and any other preferred languages that we want to use (Python, C#, etc.) must be installed. However, for my purposes here, the minimum requirements are Node.js and Typescript.
        - content: |
            Using the Node Package Manager(npm) in global mode allows me to install Typescript which will be used for this project.
          imgs:
            - /assets/project-images/cdk-images/cdk-1-typescript.png
        - content: |
            Now the Cloud Developement Kit can be installed. To verify the installation, I can run a 'cdk --version' check. Additionally, a pip dependencies overview let's us see exactly what the CDK has installed on top of our previously existing dependencies.
          imgs:
            - /assets/project-images/cdk-images/cdk-2-checkinstall.png

    - title: Bootstrapping and Initialization
      tabTitle: Bootstrap
      subsections:
        - content: |
            Once the CDK is installed, one last step is required before I can initialize the project and start coding in some infrastructure. The account and region needs to be Bootstrapped to the CDK, which is essentially the process where containers are created behind the scenes that will contain the necessary assets and files that Cloudformation will use when at the deployment stage. Any additional accounts or regions I wish to use must be bootstrapped as well.<br><br>

            Once the bootstrapping stage is complete, I will be able to initialize a new project and start defining infrastructure.
        - content: |
            The bootstrap process is seen here using my role's account number and region. This may be done mulitple times for additional accounts and regions. Here, CloudFormation is creating the stack preliminary resources, policies and repository that will be used with my project.
          imgs:
            - /assets/project-images/cdk-images/cdk-3-bootstrap.png
        - content: |
            Once completed, I am now ready to begin initializing the project. For my purposes, a simple blank typescript project is created. I am going to add resources directly from the IDE which will then trigger Cloudformation to add those resources to the stack.
          imgs:
            - /assets/project-images/cdk-images/cdk-4-init.png
        - content: |
            This is the file structure that the initialization process created. The key elemements here are:
          listItems:
            - "<span style='color: blue;'>cdk-demo.ts</span>: This is where I am going to create and load all of the stacks that I define."
            - "<span style='color: blue;'>cdk-demo-stack.ts</span>: This is where I actually define the stack, including reasources and properties."
            - "<span style='color: blue;'>package.json</span>: Dependencies and build scripts are defined here (npm watch, npm build, npm test)."
            - "<span style='color: blue;'>cdk.json</span>: Finally, this instructs the toolkit how to run your application."
          imgs:
            - /assets/project-images/cdk-images/cdk-5-file-structure.png
        - content: |
            We can now see confirmation that our cloudformation stack has been created by signing into the console. From here I can see everything that has been created from the CDK.
          imgs:
            - /assets/project-images/cdk-images/cdk-6-init-cf.png
        - content: |
            In addition, I can go into the template designer to get a visual representation of what preliminary resources and roles were created in the stack.
          imgs:
            - /assets/project-images/cdk-images/cdk-7-designer.png

    - title: Defining AWS Infrastructure as Code
      tabTitle: IaC
      subsections:
        - content: |
            At this point I have all of the local dependencies installed and the project stack has been initialized. Now I can begin to define the infrastructure as code using TypeScript which will later be compiled into JavaScript and verified. I then deploy the infrastructure that is defined in the final step. In this project, the created stack consists of a VPC with 3 public subnets.
        - content: |
            This preliminary measure, while optional, will define the environment for our stack using the account and region that the stack will be deployed to. If I skip this step, the stack will be considered environment-agnostic which will allow it to be deployed to any environment, but certain features and context lookups will be unavailable(eg. ec2.Vpc.fromLookup).
          imgs:
            - /assets/project-images/cdk-images/cdk-8-acct-reg.png
        - content: |
            In order to actually start defining the infrastructure, I must first install the EC2 construct library module.   A construct is essentially a dependency for the CDK that represents a single or multiple AWS resources. The EC2 construct module contains support for AWS VPCs. Using Node Package Manager(npm install @aws-cdk/aws-ec2), I can install the module that will allow me to define the VPC and subnets in the stack. I then import the module into the project.<br><br>

            Now I essentially have a working skeleton for the stack definition. Were I to build and deploy the CDK now, no resources would be added. In the final step I will actually define the resources.
          imgs:
            - /assets/project-images/cdk-images/cdk-9-skeleton.png
        - content: |
            As seen here, I define a VPC with a maximum of 3 Availability Zones. The 3 Availability Zones will each have a public subnet. In order for the subnets to be properly defined, they must be configured. The subnet mask is set to /24 which will give us 254 usable IP addresses for each subnet. Now that the VPC and subnets are defined, I can build and deploy the stack in the next step.
          imgs:
            - /assets/project-images/cdk-images/cdk-10-vpc-config.png

    - title: Building and Deploying the Stack
      tabTitle: Deploying
      subsections:
        - content: |
            The configuration is now completed to deploy the stack. There is a lot going on behind the scenes and through simple defined infrastructure and one can really start to see the power of the Cloud Development Kit through . In this process I am also compiling the Typescript into JavaScript and then deploying the code as JavaScript. This will also verify that the infrastructure has been properly defined and that I haven't made any mistakes along the way.
        - content: |
            Using NPM, the code is compiled into JavaScript and checked for validity. After checking that no errors are present, I can use the CDK to deploy the stack. The change to the stack is seen here initializing and preparing to deploy.
          imgs:
            - /assets/project-images/cdk-images/cdk-11-build-deploy.png
        - content: |
            Stack resources are being created during deployment.
          imgs:
            - /assets/project-images/cdk-images/cdk-12-build-deploy-result.png
        - content: |
            In addition to viewing the change set progress in the IDE, I can log into the CloudFormation via the console and view the stack as the changes are in progress. I can confirm here that stack is successfully deployed.
          imgs:
            - /assets/project-images/cdk-images/cdk-13-build-deploy-in-progress.png
        - content: |
            As another visual aid, I logged into the console to check that all of the resources that I defined were properly deployed. Here I can also check that the VPC is available...
          imgs:
            - /assets/project-images/cdk-images/cdk-14-result-vpc.png
            - /assets/project-images/cdk-images/cdk-15-result-subnets.png
        - content: |
            ...and the public subnets are properly deployed, one in each of the 3 Availability zones. Each has a subnet mask of /24 and the necessary route tables and internet gateways defined. The stack has now been successfully deployed and I can deploy more change sets to the stack via code to add or remove resources as needed.

    - title: Summary
      tabTitle: Summary
      subsections:
        - content: |
            For me, this project really highlighted the power and utility of the Cloud Development Kit. It is an extremely powerful tool, being able to define and deploy entire stacks of CloudFormation-generated
            infrastructure using some code and your favorite IDE. As an alternative to CloudFormation templates, some may prefer this as the infrastructure can be defined using a language that some feel either
            more comfortable with or proficient in. 
            <br><br>
            The ease at which you can provision and destroy architectures could have great applications for copying entire infrastructures over to another region for higher availability and resiliency, 
            and in a disaster recovery plan to automate recovery to reduce RPO and RTO for less downtime.
            <br><br>
            Overall, the CDK is a great tool, a great alternative to CloudFormation templating and a very useful weapon to have in one's cloud arsenal.
          imgs:
